---
title: "GroupProject-InterimReport"
author: "Sai Deepthi Mattam, Sri Harsha Samanthula"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

### INTRODUCTION

  The client's requirement is a real-time effective model to predict final selling price of houses in the city of Ames, Iowa.

### OBJECTIVE

  Initial focus of the project is to gain knowledge of the data and understand the relation between each of the variables to the house's sale price. Later, further statistical analysis will be conducted to select any 5 variables which tend to effect the price the most.
  
  Later, using the training data set, a best-fitting model will be constructed with the 5 variables as predictors of housing prices. Performance of various statistical models will be compared against each other to determine which model fits the best.

### ABOUT THE DATA

  The data set available on Kaggle contains 80 variables that involve in assessing home values. Out of these, 20 are continuous, 14 are discrete and the remaining 46 are categorical variables. This data has been randomized and then split in to two sets(train and test) of equal size. "SalePrice" is the outcome variable



```{r ,include=F}
### REQUIRED PACKAGES
# install.packages("e1071")
# install.packages("Amelia")
# install.packages("RANN")
# install.packages("ipred")
# install.packages("corrplot")
# install.packages("RColorBrewer")
# install.packages("lars")
# install.packages("glmnet")
# install.packages("ggplot2")
# install.packages("devtools")
# install_github("easyGgplot2", "kassambara")
# install.packages("FSelector")
# install.packages("mlbench")

library(FSelector)
library(mlbench)
# library(easyGgplot2)
# library(devtools)
library(ggplot2)
library(glmnet)
library(lars)
library(RColorBrewer)
library(reshape2)
library(ggplot2)
library(e1071)
library(dplyr)
library(Amelia)
library(RANN)
library(arm)
library(caret)
library(ipred)
library(corrplot)
library(knitr)

```


```{r, include=F}
### DEFINING USEFUL FUNCTIONS
rmse <- function(y, yhat) {
  sqrt(mean((y - yhat)^2))
}

```

### DATA EXPLORATION

```{r Loading the data, include=F}

# setwd('/Users/dmatam/Google Drive/1_PC/Predictive Analytics with R/Final Project/Raw Data/')
setwd("E:/MS/PAR/GroupProject")
# setwd('/Users/mdobbali/Google Drive/University/Classes/Pred with R/Data/')


dt.train <- read.csv('train.csv', stringsAsFactors = FALSE)
dt.test <- read.csv('test.csv')
dim(dt.train)
str(dt.train)


```

Certain columns have missing values(NAs). Below is the summary of all missing value information.

```{r Summarizing information on missing values, include=T,echo=F}

mis.vars <-data.frame(colSums(sapply(dt.train, is.na)))
colnames(mis.vars) <- c('No_of_NAs')
kable(subset(mis.vars, No_of_NAs > 0 ))

```

```{r, include=F}

mean_sp <- mean(dt.train$SalePrice)
median_sp <- median(dt.train$SalePrice)
sd_sp <- sd(dt.train$SalePrice)

sale_price_stats <- data.frame(mean_sp, median_sp, sd_sp)
```

### DATA VISUALIZATION

Summary statistics of Sales Price
```{r, include=T,echo=F}

sale_price_stats

sale_price <- data.frame(dt.train$SalePrice)

hist(sale_price$dt.train.SalePrice,xlim = c(1000,800000),main = "Sale Price Distribution",xlab = 'Sale Price',freq = FALSE,col=brewer.pal(8,"Set3"),las = 3,breaks = 190)
lines(density(sale_price$dt.train.SalePrice))

```

This histogram clearly shows that distribution of SalesPrice is Skewed to the right. To rectify this we need to apply log or power functions to SalesPrice variable.

```{r, include=T,echo=F}

hist(log(sale_price$dt.train.SalePrice),main = "Log of Sale Price Distribution",xlab = 'Sale Price',freq = FALSE,col=brewer.pal(8,"Set3"),las = 3,breaks = 190)
lines(density(log(sale_price$dt.train.SalePrice)))

```


```{r, include=F}
numeric_var <- names(dt.train)[which(sapply(dt.train, is.numeric))]
df.corr <- data.frame(cor(dt.train[,(numeric_var)], method="pearson"))
#View(df.corr)
typeof(df.corr)

#Correlation with Each variables and Sale Price:
df.sale.corr <- data.frame(abs(df.corr[,38]))
df.sale.corr$features <- names(df.corr)
#View(df.sale.corr)

df.sale.corr<- df.sale.corr[order(-df.sale.corr$abs.df.corr...38..),]
#View(df.sale.corr) #Ordered list of Correlations

#Top 5 correlated features
top5Corr <- df.sale.corr[2:7,]

```

Top 5 Correlation Numerical Variables

```{r , include=T,echo=F}

colnames(top5Corr)<- c("Cors","Features")

kable(top5Corr)

```




Exploring features using Scatterplots, BoxPlots etc

```{r , include=T,echo=F}

#Scatter Plots for Numerical 

options(scipen=5)
plot(dt.train$GrLivArea, dt.train$SalePrice, main="Scatterplot: GrLivArea vs SalePrice", 
  	xlab="Garage Liv Area ", ylab="Sales Price ", pch=20, col = "red")
abline(lm(dt.train$SalePrice~dt.train$GrLivArea), col="black") # regression line 

plot(dt.train$GarageArea, dt.train$SalePrice, main="Scatterplot: GarageArea vs SalePrice", 
  	xlab="Garage Area ", ylab="Sales Price ", pch=20, col = "red")
abline(lm(dt.train$SalePrice~dt.train$GarageArea), col="black") # regression line

plot(dt.train$TotalBsmtSF, dt.train$SalePrice, main="Scatterplot: TotalBsmtSF vs SalePrice", 
  	xlab="TotalBsmtSF ", ylab="Sales Price ", pch=20, col = "red",xlim = c(1, 3000))
abline(lm(dt.train$SalePrice~dt.train$TotalBsmtSF), col="black") # regression line

plot(dt.train$X1stFlrSF, dt.train$SalePrice, main="Scatterplot: X1stFlrSF vs SalePrice", 
  	xlab="X1stFlrSF ", ylab="Sales Price ", pch=20, col = "red",xlim = c(250, 2500))
abline(lm(dt.train$SalePrice~dt.train$X1stFlrSF), col="black") # regression line


plot(dt.train$YearBuilt, dt.train$SalePrice, main="Scatterplot: GrLivArea vs SalePrice", 
  	xlab="Year Built ", ylab="Sales Price ", pch=20, col = "red")
abline(lm(dt.train$SalePrice~dt.train$YearBuilt), col="black") # regression line 


#Categorical Varaible

p <- ggplot(dt.train, aes(factor(OverallQual), SalePrice))
p + geom_violin(scale = "width")
p + geom_violin(fill = "yellow", colour = "red")


boxplot(dt.train$SalePrice~dt.train$OverallCond, data=dt.train, notch=FALSE, 
  col=(c("gold","darkgreen")),
  main="Overall House Condition and Price", xlab="Overall Quality")

boxplot(dt.train$SalePrice~dt.train$GarageCars, data=dt.train, notch=FALSE, 
  col=(c("gold","red")),
  main="Garage Cars and Price", xlab="Overall Quality")

barplot(table(dt.train$Neighborhood),width = 0.5, main="Neighbourhood Distribution", las = 3,axisnames = TRUE, col = "Red", border = "yellow")


dt.train$bathrooms <- dt.train$FullBath + 0.5*dt.train$HalfBath
boxplot(dt.train$SalePrice~dt.train$bathrooms, data =dt.train, notch = FALSE,col=(c("gold","red")),
  main="Bathrooms and Sales price", xlab="Total Bathrooms")

```

### DATA CLEANING

NAs in numeric variables: Since these variables have an impact on the outcome variables, they can not be ignored. Also, the number of missing values for each variable is significantly higher which might introduce a substantial amount of bias or create reductions in efficiency. To avoid this, Imputation has been performed and Include
methods on these variables. Imputation is a process of replacing missing data with an estimated value based on other available information.

```{r Performing Imputation on numeric variables3, include=F}

aimp <- amelia(dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))], m = 25)

summary(aimp)

```

Imputation with Amelia.As Amelia is known for better efficiency and reduction in bias when compared to Mean imputations, it has been used. 

```{r Performing Imputation on numeric variables1, include=T,echo=F}
plot(aimp)


```

```{r Performing Imputation on numeric variables2, include=F}
# head(aimp$imputations[[1]])

#aimp$imputations$imp25$LotFrontage

bagImpute <- predict(preProcess(dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))], method = c("bagImpute")), dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))])
bagImpute

dt.train$GarageYrBlt <- round(bagImpute$GarageYrBlt)
dt.train$MasVnrArea <- bagImpute$MasVnrArea
dt.train$LotFrontage <- bagImpute$LotFrontage

```
Here, out of 80 varaibles, there are only 3 variables that has missing values. Single imputations works well in this case. So, we used Bagimpute 

NAs in character variables: All character variables contain the category of a certain feature available in the house. As per the data description from Kaggle, NAs in such cases means absence of that feature. Hence, replacing NAs with more descriptive words.

```{r Handling NAs in character variables, include=F}

dt.train$Alley <- ifelse(is.na(dt.train$Alley),"No Alley", dt.train$Alley)

dt.train$MasVnrType <- ifelse(is.na(dt.train$MasVnrType),"None", dt.train$MasVnrType)
# some data has masvnrtype none and area <> 0
#subset(dt.train, dt.train$MasVnrType == "None")$MasVnrArea 

dt.train$BsmtQual <- ifelse(is.na(dt.train$BsmtQual),"NoBsmt", dt.train$BsmtQual)

dt.train$BsmtCond <- ifelse(is.na(dt.train$BsmtCond),"NoBsmt", dt.train$BsmtCond)

dt.train$BsmtExposure <- ifelse(is.na(dt.train$BsmtExposure),"NoBsmt", dt.train$BsmtExposure)

dt.train$BsmtFinType1 <- ifelse(is.na(dt.train$BsmtFinType1),"NoBsmt", dt.train$BsmtFinType1)

dt.train$BsmtFinType2 <- ifelse(is.na(dt.train$BsmtFinType2),"NoBsmt", dt.train$BsmtFinType2)

dt.train$FireplaceQu <- ifelse(is.na(dt.train$FireplaceQu),"NoFireplace", dt.train$FireplaceQu)

dt.train$GarageType <- ifelse(is.na(dt.train$GarageType),"NoGarage", dt.train$GarageType)

dt.train$GarageFinish <- ifelse(is.na(dt.train$GarageFinish),"NoGarage", dt.train$GarageFinish)

dt.train$GarageQual <- ifelse(is.na(dt.train$GarageQual),"NoGarage", dt.train$GarageQual)

dt.train$GarageCond <- ifelse(is.na(dt.train$GarageCond),"NoGarage", dt.train$GarageCond)

dt.train$GarageCond <- ifelse(is.na(dt.train$GarageCond),"NoGarage", dt.train$GarageCond)

dt.train$PoolQC <- ifelse(is.na(dt.train$PoolQC),"NoPool", dt.train$PoolQC)

dt.train$Fence <- ifelse(is.na(dt.train$Fence),"NoFence", dt.train$Fence)

dt.train$MiscFeature <- ifelse(is.na(dt.train$MiscFeature),"NoFence", dt.train$MiscFeature)

```



```{r Calcualting number of NAs in all the variables, include=F}

# Observe that all the NAs have been replaced meaningfully
kable(data.frame(colSums(sapply(dt.train, is.na))))

```

Viewing the Correlation Plot after 

```{r Correlation Plot, include=T,echo=F}

correlations <- cor(dt.train[, numeric_var], use = "everything")
corrplot(correlations, method = "circle", type="lower",  sig.level = 0.01, insig = "blank", tl.col = "grey",tl.cex = 0.6)

```

Inspecting Multicolinearity between features in order to eliminate highly corelated features.

```{r inspecting multicolinearity, include=F}

df.corr <- data.frame(cor(dt.train[,(numeric_var)], method="pearson"))

df.mul.cor <- NULL 
for(i in 1:nrow(df.corr)){
  for(j in 1:i){
    df.temp <- NULL
    if(!is.na(df.corr[i,j])){
      if(df.corr[i,j] >= 0.6 && df.corr[i,j] != 1){
        df.temp$name1 <- names(df.corr)[i]
        df.temp$name2 <- names(df.corr)[j]
        df.temp$cor <- df.corr[i,j]
        df.mul.cor <- rbind(df.mul.cor,df.temp)
      }
    }
  }
}
df.mul.cor <- as.data.frame(df.mul.cor)
```

```{r, include=T, echo=F}

df.mul.cor <- subset(df.mul.cor, name1 != "SalePrice")
kable(df.mul.cor, row.names=FALSE)

```

Converting character variables into factors/catergorical variables.  

```{r Factorizing variables, include=F}

char_var <- names(dt.train)[which(sapply(dt.train, is.character))]

# allCateg_var <- c(char_var, 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'TotRmsAbvGrd', 'Fireplaces')

numeric_var <- names(dt.train)[which(sapply(dt.train, is.numeric))]
# allNumeric_var <- setdiff(numeric_var, c( 'OverallQual', 'OverallCond', 'YearBuild', 'YearRemodAdd','TotRmsAbvGrd','Fireplaces'))

for(name in char_var){
  #print(name)
  dt.train[[name]] <- factor(dt.train[[name]])
}

```

### MODEL AND MODEL DEVELOPMENT

Creating a base Linear Model using all the predictors.

```{r, include=T,echo=T}

lm.all <- standardize(
  lm(
    SalePrice ~ MSSubClass +   MSZoning +     LotFrontage +  LotArea +      Street +      
      Alley +        LotShape +    
    LandContour +  Utilities +    LotConfig +    LandSlope +    Neighborhood + Condition1 + 
      Condition2 +   BldgType +    
    HouseStyle +   OverallQual +  OverallCond +  YearBuilt +    YearRemodAdd + RoofStyle +   
      RoofMatl +     Exterior1st + 
    Exterior2nd +  MasVnrType +   MasVnrArea +   ExterQual +    ExterCond +    Foundation +  
      BsmtQual +     BsmtCond +    
    BsmtExposure + BsmtFinType1 + BsmtFinSF1 +   BsmtFinType2 + BsmtFinSF2 +   BsmtUnfSF +  
      TotalBsmtSF +  Heating +     
    HeatingQC +    CentralAir +   Electrical +   X1stFlrSF +    X2ndFlrSF +    LowQualFinSF 
    + GrLivArea +    BsmtFullBath +
    BsmtHalfBath + FullBath +     HalfBath +     BedroomAbvGr + KitchenAbvGr + KitchenQual
    +  TotRmsAbvGrd + Functional +  
    Fireplaces +   FireplaceQu +  GarageType +   GarageYrBlt +  GarageFinish + GarageCars
    +   GarageArea +   GarageQual +  
    GarageCond +   PavedDrive +   WoodDeckSF +   OpenPorchSF +  EnclosedPorch + X3SsnPorch 
    +   ScreenPorch +  PoolArea +    
    PoolQC +       Fence +        MiscFeature +  MiscVal +      MoSold +       YrSold +  
      SaleType +     SaleCondition
    , data = dt.train
  )
)
```

```{r, include=F}

summary(lm.all)

coefs <-  as.data.frame(summary(lm.all)$coefficients)
coefs$vars <-  rownames(coefs)
coefs$estabs <- abs(coefs$Estimate)
coefs.20 <- coefs[order(-coefs$estabs, -coefs$`Pr(>|t|)`),][1:5,]

ggplot(coefs.20, aes(vars, Estimate)) + 
  geom_hline(yintercept=0, lty=2, lwd=1, colour="grey50") +
  geom_point(size=4, pch=21, fill="yellow") +
  theme_bw()

```

```{r, include=T,echo=F}

# coefplot(lm.all)


cat("RMSE of the baseline model with all predictors ", rmse(dt.train$SalePrice, predict(lm.all)))

```

Removing the predictor with NAs as coeffiecient, because of multi colinearity
Exterior2nd, BsmtCond,  BsmtFinType1, TotalBsmtSF, Electrical, GarageFinish, GarageCond, GrLivArea, GarageQual

```{r, include=T, echo=T}
lm.sel <- standardize(
  lm(
    SalePrice ~ MSSubClass +   MSZoning +     LotFrontage +  LotArea +      Street + 
      Alley +        LotShape +    
    LandContour +  Utilities +    LotConfig +    LandSlope +    Neighborhood +
      Condition1 +   Condition2 +   BldgType +    
    HouseStyle +   OverallQual +  OverallCond +  YearBuilt +    YearRemodAdd +
      RoofStyle +    RoofMatl +     Exterior1st + 
    # 
      MasVnrType +   MasVnrArea +   ExterQual +    ExterCond +    Foundation +   BsmtQual +    
    BsmtExposure  + BsmtFinSF1 +   BsmtFinType2 + BsmtFinSF2 +   BsmtUnfSF      +  Heating +     
    HeatingQC +    CentralAir +     X1stFlrSF +    X2ndFlrSF +   
      LowQualFinSF  +    BsmtFullBath +
    BsmtHalfBath + FullBath +     HalfBath +     BedroomAbvGr + KitchenAbvGr
    + KitchenQual +  TotRmsAbvGrd + Functional +  
    Fireplaces +   FireplaceQu +  GarageType +   GarageYrBlt +  
      GarageCars +   GarageArea  +  
     PavedDrive +   WoodDeckSF +   OpenPorchSF +  EnclosedPorch + X3SsnPorch + 
      ScreenPorch +  PoolArea +    
    PoolQC +       Fence +        MiscFeature +  MiscVal +      MoSold + 
      YrSold +       SaleType +     SaleCondition
    , data = dt.train
  )
)

```


```{r, include=T,echo=F}

cat("RMSE of the model after removing multicollinear variables with all predictors ", rmse(dt.train$SalePrice, predict(lm.sel)))

```

Picking predictors basing on the Beta coeffiencients and P values.

```{r, include=F}

lm.df <- as.data.frame(coef(summary(lm.sel)))

names(lm.df)
lm.df$estabs <- abs(lm.df$Estimate)

```


```{r, include=T,echo=F}

kable(lm.df[order(-lm.df$estabs, -lm.df$`Pr(>|t|)`),][1:20,])

```

New model with just the strong predictors picked from above, and strongly corelated variables. 

```{r, include=T,echo=F}

lm.sel2 <- lm(SalePrice ~ RoofMatl+Condition2+PoolQC+OverallQual+RoofStyle+OverallCond+YearBuilt+GarageArea+GrLivArea+TotalBsmtSF,data=dt.train)

# summary(lm.sel2)

cat("RMSE of the model with selected variables", rmse(dt.train$SalePrice, predict(lm.sel2)))  

```

Using FSelector, and performing Chisquare test to pick important features.

```{r, include=F,echo=F}

weights <- chi.squared(SalePrice~., dt.train)
print(weights)
subset <- cutoff.k(weights, 5)
f <- as.simple.formula(subset, "SalePrice")

lm.sel3 <- lm(f,data=dt.train)

summary(lm.sel3)

cat("RMSE of the model with selected variables", rmse(dt.train$SalePrice, predict(lm.sel3)))  
```

Features obtained:
```{r, include=T,echo=F}
print(f)

```

Using CFS test to pick important numercial variables.

```{r, include=F}
subset <- cfs(SalePrice~., dt.train)
f <- as.simple.formula(subset, "SalePrice")


```
Features obtained.
```{r, include=T,echo=F}
print(f)
```

For Feature selections we used chi.squared which will find weights of discrete attributes.This shows us the most important features out of all available variables. The features obtained according to this test are :  OverallQual, FullBath, Neightbourhood, Fireplace, GarageCar . So, these are most influential categorical variables. Correlation based feature selection has also been used to identity the most important numerical variables. Numerical variables obtained in this test are : Overall Qual, GarageCar,   TotalBasment,  GrLivArea

Final Model with just the Top 5 predictors.

```{r, include=T,echo=T}
lm.sel4 <- standardize(lm(SalePrice ~ OverallQual  + TotalBsmtSF 
                          + GrLivArea + GarageCars + Neighborhood ,data=dt.train))

```

```{r, include=T,echo=F}

# summary(lm.sel4)
cat("RMSE of the final model", rmse(dt.train$SalePrice, predict(lm.sel4)))  

```

After brainstorming about general features considered by people to make a decision about a house, conclusion have been made that above features are considered more often than other available variables

Exploring the residual plot of the final model

```{r, include=T,echo=F}
plot(lm.sel4,which = 1)
```

including the quadratic term of Quality variables to address the non linearity

```{r, include=F,echo=F}
lm.sel5 <- standardize(lm(SalePrice ~ OverallQual + I(OverallQual^2) + TotalBsmtSF + GrLivArea  + GarageCars + Neighborhood + Neighborhood:GrLivArea ,data=dt.train))
```

```{r, include=T, echo=T}
cat("RMSE of the final model with quadratic term and interaction", rmse(dt.train$SalePrice, predict(lm.sel5)))  
```

Residual plot of the final model after adding the quadratic variable and interaction term

```{r, include=T,echo=F}

plot(lm.sel5,which = 1)

```

### NEXT STEPS
 After the initial attempts and computations, these following steps have been planned to improve the model
 
1. Use ensemble to improve the model performance
2. Try various combinatons of interactions between variables and try building model with various forms such as quadratic, power forms.
