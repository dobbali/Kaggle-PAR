---
title: "GroupProject-InterimReport"
author: "Sai Deepthi Matam, Sri Harsha Samanthula"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

### INTRODUCTION

  The primary requirement is a real-time effective model to predict final selling price of houses in the city of Ames, Iowa.

### OBJECTIVE

  Initial focus of the project is to gain knowledge of the data and understand the relation between each of the variables to the house's sale price. Later, further statistical analysis will be conducted to select any 5 variables which tend to effect the price the most.
  
  Later, using the training data set, a best-fitting model will be constructed with the 5 variables as predictors of housing prices. Performance of various statistical models will be compared against each other to determine which model fits the best.

### ABOUT THE DATA

  The data set available on Kaggle contains 80 variables that involve in assessing home values. Out of these, 20 are continuous, 14 are discrete and the remaining 46 are categorical variables. This data has been randomized and then split in to two sets(train and test) of equal size. "SalePrice" is the outcome variable



```{r ,include=F}
### REQUIRED PACKAGES
# install.packages("e1071")
# install.packages("Amelia")
# install.packages("RANN")
# install.packages("ipred")
# install.packages("corrplot")
# install.packages("RColorBrewer")
# install.packages("lars")
# install.packages("glmnet")
# install.packages("ggplot2")
# install.packages("devtools")
# install_github("easyGgplot2", "kassambara")
# install.packages("FSelector")
# install.packages("mlbench")

library(FSelector)
library(mlbench)
# library(easyGgplot2)
# library(devtools)
library(ggplot2)
library(glmnet)
library(lars)
library(RColorBrewer)
library(reshape2)
library(ggplot2)
library(e1071)
library(dplyr)
library(Amelia)
library(RANN)
library(arm)
library(caret)
library(ipred)
library(corrplot)
library(knitr)

```


```{r, include=F}
### DEFINING USEFUL FUNCTIONS
rmse <- function(y, yhat) {
  sqrt(mean((y - yhat)^2))
}

```


```{r Loading the data, include=F}

# setwd('/Users/dmatam/Google Drive/1_PC/Predictive Analytics with R/Final Project/Raw Data/')
setwd("E:/MS/PAR/GroupProject")
setwd('/Users/mdobbali/Google Drive/University/Classes/Pred with R/Data/')


dt.train <- read.csv('train.csv', stringsAsFactors = FALSE)
dt.test <- read.csv('test.csv')
dim(dt.train)
str(dt.train)


```

Certain columns have missing values(NAs). Below is the summary of all missing value information.

```{r Summarizing information on missing values, include=T,echo=F}

mis.vars <-data.frame(colSums(sapply(dt.train, is.na)))
colnames(mis.vars) <- c('No_of_NAs')
kable(subset(mis.vars, No_of_NAs > 0 ))

```

```{r, include=F}

mean_sp <- mean(dt.train$SalePrice)
median_sp <- median(dt.train$SalePrice)
sd_sp <- sd(dt.train$SalePrice)

sale_price_stats <- data.frame(mean_sp, median_sp, sd_sp)
```

### DATA CLEANING

NAs in numeric variables: Since these variables have an impact on the outcome variables, they can not be ignored. Also, the number of missing values for each variable is significantly higher which might introduce a substantial amount of bias or create reductions in efficiency. To avoid this, Imputation has been performed and Include
methods on these variables. Imputation is a process of replacing missing data with an estimated value based on other available information.

```{r Performing Imputation on numeric variables3, include=F}

aimp <- amelia(dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))], m = 25)

summary(aimp)

```

Imputation with Amelia.As Amelia is known for better efficiency and reduction in bias when compared to Mean imputations, it has been used. 

```{r Performing Imputation on numeric variables1, include=T,echo=F}
plot(aimp)

par(mfrow = c(1,1))
```

```{r Performing Imputation on numeric variables2, include=F}
# head(aimp$imputations[[1]])

#aimp$imputations$imp25$LotFrontage

bagImpute <- predict(preProcess(dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))], method = c("bagImpute")), dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))])
bagImpute

dt.train$GarageYrBlt <- round(bagImpute$GarageYrBlt)
dt.train$MasVnrArea <- bagImpute$MasVnrArea
dt.train$LotFrontage <- bagImpute$LotFrontage

```

Here, out of 80 varaibles, there are only 3 variables that has missing values. Single imputations works well in this case. So, we used Bagimpute 

NAs in character variables: All character variables contain the category of a certain feature available in the house. As per the data description from Kaggle, NAs in such cases means absence of that feature. Hence, replacing NAs with more descriptive words.

```{r Handling NAs in character variables, include=F}

dt.train$Alley <- ifelse(is.na(dt.train$Alley),"No Alley", dt.train$Alley)

dt.train$MasVnrType <- ifelse(is.na(dt.train$MasVnrType),"None", dt.train$MasVnrType)
# some data has masvnrtype none and area <> 0
#subset(dt.train, dt.train$MasVnrType == "None")$MasVnrArea 

dt.train$BsmtQual <- ifelse(is.na(dt.train$BsmtQual),"NoBsmt", dt.train$BsmtQual)

dt.train$BsmtCond <- ifelse(is.na(dt.train$BsmtCond),"NoBsmt", dt.train$BsmtCond)

dt.train$BsmtExposure <- ifelse(is.na(dt.train$BsmtExposure),"NoBsmt", dt.train$BsmtExposure)

dt.train$BsmtFinType1 <- ifelse(is.na(dt.train$BsmtFinType1),"NoBsmt", dt.train$BsmtFinType1)

dt.train$BsmtFinType2 <- ifelse(is.na(dt.train$BsmtFinType2),"NoBsmt", dt.train$BsmtFinType2)

dt.train$FireplaceQu <- ifelse(is.na(dt.train$FireplaceQu),"NoFireplace", dt.train$FireplaceQu)

dt.train$GarageType <- ifelse(is.na(dt.train$GarageType),"NoGarage", dt.train$GarageType)

dt.train$GarageFinish <- ifelse(is.na(dt.train$GarageFinish),"NoGarage", dt.train$GarageFinish)

dt.train$GarageQual <- ifelse(is.na(dt.train$GarageQual),"NoGarage", dt.train$GarageQual)

dt.train$GarageCond <- ifelse(is.na(dt.train$GarageCond),"NoGarage", dt.train$GarageCond)

dt.train$GarageCond <- ifelse(is.na(dt.train$GarageCond),"NoGarage", dt.train$GarageCond)

dt.train$PoolQC <- ifelse(is.na(dt.train$PoolQC),"NoPool", dt.train$PoolQC)

dt.train$Fence <- ifelse(is.na(dt.train$Fence),"NoFence", dt.train$Fence)

dt.train$MiscFeature <- ifelse(is.na(dt.train$MiscFeature),"NoFence", dt.train$MiscFeature)

```



```{r Calcualting number of NAs in all the variables, include=F}

# Observe that all the NAs have been replaced meaningfully
kable(data.frame(colSums(sapply(dt.train, is.na))))

```



### DATA VISUALIZATION

To understand the spread of the Sale Price of houses in Ames.  

```{r, include=T,echo=F}

cat("Mean : " , sale_price_stats[,1])
cat("Median : " , sale_price_stats[,2])
cat("Standard Deviation : " , sale_price_stats[,3])
```

Here the Mean > Median which indicates a right skew in the data. The same is also plotted below:

```{r, include=T, echo=F}

sale_price <- data.frame(dt.train$SalePrice)

hist(sale_price$dt.train.SalePrice,xlim = c(1000,800000),main = "Sale Price Distribution",xlab = 'Sale Price',freq = FALSE,col=brewer.pal(8,"Set3"),las = 3,breaks = 190)
lines(density(sale_price$dt.train.SalePrice))

```

This histogram clearly shows that distribution of SalesPrice is Skewed to the right. To rectify this we need to apply log or power functions to SalesPrice variable. 

```{r, include=T,echo=F}

hist(log(sale_price$dt.train.SalePrice),main = "Log of Sale Price Distribution",xlab = 'Sale Price',freq = FALSE,col=brewer.pal(8,"Set3"),las = 3,breaks = 190)
lines(density(log(sale_price$dt.train.SalePrice)))

```

After applying the log function to the SalePrice, the distribution is closer to a normal distribution. Hence we can apply central limit theorm. 

```{r, include=F}
numeric_var <- names(dt.train)[which(sapply(dt.train, is.numeric))]
df.corr <- data.frame(cor(dt.train[,(numeric_var)], method="pearson"))

#Correlation with Each variables and Sale Price:
df.sale.corr <- data.frame(abs(df.corr[,38]))
df.sale.corr$features <- names(df.corr)
#View(df.sale.corr)

df.sale.corr<- df.sale.corr[order(-df.sale.corr$abs.df.corr...38..),]
#View(df.sale.corr) #Ordered list of Correlations

#Top 5 correlated features
top5Corr <- df.sale.corr[2:7,]

```

Top 5 Correlation Numerical Variables

```{r , include=T,echo=F}

colnames(top5Corr)<- c("Cors","Features")
row.names(top5Corr) <- NULL

kable(top5Corr[,c(2,1)])

```

Exploring top 5 correlated features using Scatterplots, BoxPlots etc

```{r , include=T,echo=F}

#Scatter Plots for Numerical 
par(mfrow=c(2,1))

options(scipen=5)

plot(dt.train$GrLivArea, dt.train$SalePrice, main="Scatterplot: GrLivArea vs SalePrice", 
  	xlab="Above grade (ground) living area square feet", ylab="Sales Price ", pch=20, col = "red")
abline(lm(dt.train$SalePrice~dt.train$GrLivArea), col="black") # regression line 
print("This plot clearly shows that the Living area above grade has a strong positive linear relationship with the Sale price.")

plot(dt.train$GarageArea, dt.train$SalePrice, main="Scatterplot: GarageArea vs SalePrice", 
  	xlab="Garage Area ", ylab="Sales Price ", pch=20, col = "red")
abline(lm(dt.train$SalePrice~dt.train$GarageArea), col="black") # regression line
print("This plot clearly shows that the Garage Area has a strong positive linear relationship with the Sale price.But, this graph has lot of data points concentrated at units '0' which results in an anomaly. There are considerable amount of houses with no basement at all. That resulted in this anomaly")


plot(dt.train$TotalBsmtSF, dt.train$SalePrice, main="Scatterplot: TotalBsmtSF vs SalePrice", 
  	xlab="TotalBsmtSF ", ylab="Sales Price ", pch=20, col = "red",xlim = c(1, 3000))
abline(lm(dt.train$SalePrice~dt.train$TotalBsmtSF), col="black") # regression line
print("This plot clearly shows that the Total Basement Area has a strong positive linear relationship with the Sale price.But, this graph has lot of data points concentrated at units '0' which results in an anomaly. There are considerable amount of houses with no basement at all. That resulted in this anomaly")


plot(dt.train$X1stFlrSF, dt.train$SalePrice, main="Scatterplot: X1stFlrSF vs SalePrice", 
  	xlab="X1stFlrSF ", ylab="Sales Price ", pch=20, col = "red",xlim = c(250, 2500))
abline(lm(dt.train$SalePrice~dt.train$X1stFlrSF), col="black") # regression line
print("This plot clearly shows that the First Floor area has a strong positive linear relationship with the Sale price.")

par(mfrow=c(1,1))
#Categorical Varaible

p <- ggplot(dt.train, aes(factor(OverallQual), SalePrice))
p + geom_violin(scale = "width")
p + geom_violin(fill = "yellow", colour = "red")
print("This violin plot shows probability density of the data at different values. For a house with maximum(10) Over all Quality has very high spread and distribution is close to normal where as Over all Quality with 2 has no standard probability and has minimum spread. Rest of the values has close to normal distribution with mean value increasing as the Over all Quality increase ")

par(mfrow=c(2,1))
boxplot(dt.train$SalePrice~dt.train$OverallCond, data=dt.train, notch=FALSE, 
  col=(c("gold","darkgreen")),
  main="Overall House Condition and Price", xlab="Overal Condition")
print("It is quiet evident that OverallCond with 5 units has many outliers and mean sales price of houses with more than 5 rating for Over all condition is similar ")

boxplot(dt.train$SalePrice~dt.train$GarageCars, data=dt.train, notch=FALSE, 
  col=(c("gold","red")),
  main="Garage Cars and Price", xlab="Garage Cars")
print("This plot shows that houses with 3 car Garage Space has suprisingly greater mean than the rest of the values ")

dt.train$bathrooms <- dt.train$FullBath + 0.5*dt.train$HalfBath
boxplot(dt.train$SalePrice~dt.train$bathrooms, data =dt.train, notch = FALSE,col=(c("gold","red")),
  main="Bathrooms and Sales price", xlab="Total Bathrooms")
print("Data given has Full and Half bathrooms. Here, we combined those columns to see data so that both full and half bathroom quantity is quantized in a single value. Box plot clearly shows that prices for each value of 1,1.5, 2 and 2.5 house prices are quite similar to each other as the width of box is short  ")

boxplot(dt.train$SalePrice~dt.train$Neighborhood, data =dt.train, notch = FALSE,col=(c("gold","red")),
  main="Neighborhood and Sales price", xlab="Neighborhood", las=2)
print(" ")


```

Viewing the Correlation Plot 

```{r Correlation Plot, include=T,echo=F}

correlations <- cor(dt.train[, numeric_var], use = "everything")
corrplot(correlations, method = "circle", type="lower",  sig.level = 0.01, insig = "blank", tl.col = "grey",tl.cex = 0.6)

print("Above Correlation heat map helps to visualize correlation between different combinations of variables")

```

Inspecting Multicolinearity between features in order to eliminate highly corelated features.

```{r inspecting multicolinearity, include=F}

df.corr <- data.frame(cor(dt.train[,(numeric_var)], method="pearson"))

df.mul.cor <- NULL 
for(i in 1:nrow(df.corr)){
  for(j in 1:i){
    df.temp <- NULL
    if(!is.na(df.corr[i,j])){
      if(df.corr[i,j] >= 0.6 && df.corr[i,j] != 1){
        df.temp$name1 <- names(df.corr)[i]
        df.temp$name2 <- names(df.corr)[j]
        df.temp$cor <- df.corr[i,j]
        df.mul.cor <- rbind(df.mul.cor,df.temp)
      }
    }
  }
}
df.mul.cor <- as.data.frame(df.mul.cor)

```

Following table contains the combinations of variables with highest correlation which has a minimum of 0.6 as corelation value. This will identify redundant predictors 

```{r, include=T, echo=F}

df.mul.cor <- subset(df.mul.cor, name1 != "SalePrice")
kable(df.mul.cor, row.names=FALSE)

```

Converting character variables into factors/catergorical variables.  

```{r Factorizing variables, include=F}

char_var <- names(dt.train)[which(sapply(dt.train, is.character))]

# allCateg_var <- c(char_var, 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'TotRmsAbvGrd', 'Fireplaces')

numeric_var <- names(dt.train)[which(sapply(dt.train, is.numeric))]
# allNumeric_var <- setdiff(numeric_var, c( 'OverallQual', 'OverallCond', 'YearBuild', 'YearRemodAdd','TotRmsAbvGrd','Fireplaces'))

for(name in char_var){
  #print(name)
  dt.train[[name]] <- factor(dt.train[[name]])
}

```

### MODEL AND MODEL DEVELOPMENT

Creating a base Linear Model using all the predictors.

```{r, include=F,echo=F}

lm.all <- standardize(
  lm(
    SalePrice ~ MSSubClass +   MSZoning +     LotFrontage +  LotArea +      Street +      
      Alley +        LotShape +    
    LandContour +  Utilities +    LotConfig +    LandSlope +    Neighborhood + Condition1 + 
      Condition2 +   BldgType +    
    HouseStyle +   OverallQual +  OverallCond +  YearBuilt +    YearRemodAdd + RoofStyle +   
      RoofMatl +     Exterior1st + 
    Exterior2nd +  MasVnrType +   MasVnrArea +   ExterQual +    ExterCond +    Foundation +  
      BsmtQual +     BsmtCond +    
    BsmtExposure + BsmtFinType1 + BsmtFinSF1 +   BsmtFinType2 + BsmtFinSF2 +   BsmtUnfSF +  
      TotalBsmtSF +  Heating +     
    HeatingQC +    CentralAir +   Electrical +   X1stFlrSF +    X2ndFlrSF +    LowQualFinSF 
    + GrLivArea +    BsmtFullBath +
    BsmtHalfBath + FullBath +     HalfBath +     BedroomAbvGr + KitchenAbvGr + KitchenQual
    +  TotRmsAbvGrd + Functional +  
    Fireplaces +   FireplaceQu +  GarageType +   GarageYrBlt +  GarageFinish + GarageCars
    +   GarageArea +   GarageQual +  
    GarageCond +   PavedDrive +   WoodDeckSF +   OpenPorchSF +  EnclosedPorch + X3SsnPorch 
    +   ScreenPorch +  PoolArea +    
    PoolQC +       Fence +        MiscFeature +  MiscVal +      MoSold +       YrSold +  
      SaleType +     SaleCondition
    , data = dt.train
  )
)


```


```{r, include=F}

summary(lm.all)

coefs <-  as.data.frame(summary(lm.all)$coefficients)
coefs$vars <-  rownames(coefs)
coefs$estabs <- abs(coefs$Estimate)
coefs.20 <- coefs[order(-coefs$estabs, -coefs$`Pr(>|t|)`),][1:5,]

ggplot(coefs.20, aes(vars, Estimate)) + 
  geom_hline(yintercept=0, lty=2, lwd=1, colour="grey50") +
  geom_point(size=4, pch=21, fill="yellow") +
  theme_bw()

```

```{r, include=T,echo=F}

# coefplot(lm.all)


cat("RMSE of the baseline model with all predictors ", rmse(dt.train$SalePrice, predict(lm.all)))

```

Base model served two purposes. 
1. This helps to compare the performance of base model with the future models and see if the there is any improvement after selecting the best variables
2. This also helped in checking collinearity between categorical variables. 


Removing the predictor with NAs as coeffiecient because of multi colinearity.
These are the predictors removed:Exterior2nd, BsmtCond,  BsmtFinType1, TotalBsmtSF, Electrical, GarageFinish, GarageCond, GrLivArea, GarageQual

```{r, include=F, echo=F}
lm.sel <- standardize(
  lm(
    SalePrice ~ MSSubClass +   MSZoning +     LotFrontage +  LotArea +      Street + 
      Alley +        LotShape +    
    LandContour +  Utilities +    LotConfig +    LandSlope +    Neighborhood +
      Condition1 +   Condition2 +   BldgType +    
    HouseStyle +   OverallQual +  OverallCond +  YearBuilt +    YearRemodAdd +
      RoofStyle +    RoofMatl +     Exterior1st + 
    # 
      MasVnrType +   MasVnrArea +   ExterQual +    ExterCond +    Foundation +   BsmtQual +    
    BsmtExposure  + BsmtFinSF1 +   BsmtFinType2 + BsmtFinSF2 +   BsmtUnfSF      +  Heating +     
    HeatingQC +    CentralAir +     X1stFlrSF +    X2ndFlrSF +   
      LowQualFinSF  +    BsmtFullBath +
    BsmtHalfBath + FullBath +     HalfBath +     BedroomAbvGr + KitchenAbvGr
    + KitchenQual +  TotRmsAbvGrd + Functional +  
    Fireplaces +   FireplaceQu +  GarageType +   GarageYrBlt +  
      GarageCars +   GarageArea  +  
     PavedDrive +   WoodDeckSF +   OpenPorchSF +  EnclosedPorch + X3SsnPorch + 
      ScreenPorch +  PoolArea +    
    PoolQC +       Fence +        MiscFeature +  MiscVal +      MoSold + 
      YrSold +       SaleType +     SaleCondition
    , data = dt.train
  )
)

```


```{r, include=T,echo=F}

cat("RMSE of the model after removing multicollinear variables with all predictors ", rmse(dt.train$SalePrice, predict(lm.sel)))

```

Picking Top 20 predictors basing on the Beta coeffiencients and P values.

```{r, include=F}

lm.df <- as.data.frame(coef(summary(lm.sel)))

names(lm.df)
lm.df$estimate_absolute_estimates <- abs(lm.df$Estimate)

```


```{r, include=T,echo=F}

kable(lm.df[order(-lm.df$estimate_absolute_estimates, -lm.df$`Pr(>|t|)`),][1:20,])

```

New model after selecting the strong predictors picked from above, and strongly corelated variables. 
RoofMatl, Condition2, PoolQC, OverallQual, RoofStyle, OverallCond, YearBuilt, GarageArea, GrLivArea, TotalBsmtSF

```{r, include=T,echo=F}

lm.sel2 <- lm(SalePrice ~ RoofMatl+Condition2+PoolQC+OverallQual+RoofStyle+OverallCond+YearBuilt+GarageArea+GrLivArea+TotalBsmtSF,data=dt.train)

# summary(lm.sel2)

cat("RMSE of the model with selected variables", rmse(dt.train$SalePrice, predict(lm.sel2)))  

```

Using FSelector, and performing Chisquare test to pick important features.

```{r, include=F,echo=F}

weights <- chi.squared(SalePrice~., dt.train)
print(weights)
subset <- cutoff.k(weights, 5)
f <- as.simple.formula(subset, "SalePrice")

lm.sel3 <- lm(f,data=dt.train)

summary(lm.sel3)

print(f)

```

Features obtained:
FullBath + Fireplaces + OverallQual + GarageCars + Neighborhood
```{r, include=T,echo=F}

cat("RMSE of the model with selected variables from chi-squared test", rmse(dt.train$SalePrice, predict(lm.sel3)))  

```

Using CFS(Correlation based Feature Selection) test to pick important numercial variables.

```{r, include=F}
subset <- cfs(SalePrice~., dt.train)
f <- as.simple.formula(subset, "SalePrice")


```
Features obtained :
OverallQual + TotalBsmtSF + GrLivArea + GarageCars
```{r, include=F,echo=F}
print(f)
```

For Feature selections we used chi.squared which will find weights of discrete attributes.This shows us the most important features out of all available variables. The features obtained according to this test are :  OverallQual, FullBath, Neightbourhood, Fireplace, GarageCar . So, these are most influential categorical variables. Correlation based feature selection has also been used to identity the most important numerical variables. Numerical variables obtained in this test are : Overall Qual, GarageCar,   TotalBasment,  GrLivArea

Final Model with just the Top 5 predictors.

```{r, include=T,echo=T}
lm.sel4 <- standardize(lm(SalePrice ~ OverallQual  + TotalBsmtSF 
                          + GrLivArea + GarageCars + Neighborhood ,data=dt.train))
summary(lm.sel4)

```

```{r, include=T,echo=F}

cat("RMSE of the final model", rmse(dt.train$SalePrice, predict(lm.sel4)))  

```

Also,After brainstorming about general features considered by people to make a decision about a house, conclusion have been made that above features are considered more often than other available variables

Exploring the residual plot of the final model

```{r, include=T,echo=F}
plot(lm.sel4,which = 1)
```

Modifying the model futher by 
  1. Converting Quality variable into factor variable to take into account the bin like effect on the SalePrice
  2. Adding a interaction between Neighborhood ad Quality
  
```{r, include=F,echo=F}
dt.train$OverallQual <- factor(dt.train$OverallQual)

lm.sel5 <- (lm(SalePrice ~ OverallQual + TotalBsmtSF + GrLivArea  + GarageCars + Neighborhood + Neighborhood:GrLivArea ,data=dt.train))
summary(standardize(lm.sel5))
dt.train$Neighborhood[2]
dt.train
summary(dt.train$GrLivArea)
dt<- NULL
dt$OverallQual <- 10
dt$Neighborhood <- "Blmngtn"
dt$TotalBsmtSF <- 0
dt$GrLivArea <- 0
dt$GarageCars <- 0
dt$OverallQual <- factor(dt$OverallQual)
dt$Neighborhood <- factor(dt$Neighborhood)
dt <- data.frame(dt)
predict(lm.sel5, newdata = dt[1,])

plot(dt.train$GrLivArea,dt.train$SalePrice)

aggregate(SalePrice~Neighborhood, data = dt.train, median)

```


```{r, include=T, echo=T}
cat("RMSE of the final model with quality as factor and interaction term", rmse(dt.train$SalePrice, predict(lm.sel5)))  
```


After analysing the model, it is evident that Neighbourhood is one of the significant factor in deciding sales price. Especially, a house in NoRidge neighbourhood with one unit more GrLivArea compared to Blmgtn results in 110061($30891-$24902+$104072) of price increase

A well known fact which is usually considered for deciding a house price is : overallqual. A house with Quality 10 cost approximately  $1,62,181 more than a Quality 1 house with all other factors being same.

One unit increase in TotalBsmtSF results in an increase of approximately $26000 increase in sales prices with all other factors being same.Number of cars in Garage has good contribution to the sales price with $17640 of increase in price with every extra cars space a house has with all other factors being same.

Residual plot of the final model after adding the quadratic variable and interaction term

```{r, include=T,echo=F}

plot(lm.sel5,which = 1)

```
Residual Plot show that the residuals and the predicted values do not follow any linear relationship. Data points are randomly distributed. This indicates that the linear model above is appropriate for the data. 


### NEXT STEPS
 After the initial attempts and computations, these following steps have been planned to improve the model
 
1. Use ensemble to improve the model performance
2. Try various combinatons of interactions between variables and try building model with various forms such as quadratic, power forms.


