#Smoother plotted curve using ma() for k = 5,10,15,20:-
plot(ma(tcount, 5))
plot(ma(tcount,10))
plot(ma(tcount,15))
plot(ma(tcount,20))
#Forecast for next two years from 2013 to 2015:-
plot(forecast(tcount),xlab="Forecast for next two years",ylab="Count")
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count")
#Forecast for next 10 days from Jan 2013:-
plot(forecast(tcasual,10),xlab="Forecast for next 10 days",ylab="Count")
#Forecast for next 30 days from Jan 2013
plot(forecast(tcasual,30),xlab="Forecast for next 30 days",ylab="Count")
plot(ma(tcasual, 5))
par(mfrow=c(2, 2))
plot(ma(tcasual,10))
plot(ma(tcasual,15))
plot(ma(tcasual,20))
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count")
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count")
getwd()
setwd('E:/MS/BIA/Group Project')
setwd('/Users/rammatta/Desktop/Bike-Sharing-Dataset')
FinalDatafile = "day.csv"
#FinalDatafile = "SlidingWindowFinal.csv"
## Read the final with predicted values
bike <- read.csv(FinalDatafile ,stringsAsFactors = FALSE)
library(forecast)
#####################################Analysis of Casual######################3333
tbike_casual <- bike[, c("dteday", "casual", "registered","cnt")]
tbike_casual$dteday<-strptime(tbike_casual$dteday, "%Y-%m-%d",tz="GMT")
str(tbike_casual)
##Create a time series with frequency 365 for casual:-
tcasual <- ts(tbike_casual$casual, start=c(2011, 1,1), frequency=365)
#Plot the time series
plot(tcasual)
par(mfrow=c(2, 2))
#Smoother plotted curve using ma() for k = 5,10,15,20:-
plot(ma(tcasual, 5))
plot(ma(tcasual,10))
plot(ma(tcasual,15))
plot(ma(tcasual,20))
#Forecast for next two years from 2013 to 2015:-
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count")
#Forecast for next 10 days from Jan 2013:-
plot(forecast(tcasual,10),xlab="Forecast for next 10 days",ylab="Count")
#Forecast for next 30 days from Jan 2013
plot(forecast(tcasual,30),xlab="Forecast for next 30 days",ylab="Count")
#Forecast for next 60 days from Jan 2013
plot(forecast(tcasual,90),xlab="Forecast for next 1 quarter (since 2013 Jan)",ylab="Count")
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count", ylim = c(0,12000))
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count", ylim = c(0,1200))
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count", ylim = c(0,500))
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count", ylim = c(0,5000))
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count", ylim = c(0,3000))
#Forecast for next 10 days from Jan 2013:-
plot(forecast(tcasual,10),xlab="Forecast for next 10 days",ylab="Count")
#Forecast for next 30 days from Jan 2013
plot(forecast(tcasual,30),xlab="Forecast for next 30 days",ylab="Count")
#Forecast for next 60 days from Jan 2013
plot(forecast(tcasual,90),xlab="Forecast for next 1 quarter (since 2013 Jan)",ylab="Count")
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count", ylim = c(0,5000))
#Forecast for next 10 days from Jan 2013:-
plot(forecast(tcasual,10),xlab="Forecast for next 10 days",ylab="Count")
#Forecast for next 30 days from Jan 2013
plot(forecast(tcasual,30),xlab="Forecast for next 30 days",ylab="Count")
#Forecast for next 60 days from Jan 2013
plot(forecast(tcasual,90),xlab="Forecast for next 1 quarter (since 2013 Jan)",ylab="Count")
plot(forecast(tcasual),xlab="Forecast for next two years",ylab="Count", ylim = c(0,4000))
#Forecast for next 10 days from Jan 2013:-
plot(forecast(tcasual,10),xlab="Forecast for next 10 days",ylab="Count")
#Forecast for next 30 days from Jan 2013
plot(forecast(tcasual,30),xlab="Forecast for next 30 days",ylab="Count")
#Forecast for next 60 days from Jan 2013
plot(forecast(tcasual,90),xlab="Forecast for next 1 quarter (since 2013 Jan)",ylab="Count")
FinalDatafileHourly = "hour.csv"
#FinalDatafile = "SlidingWindowFinal.csv"
## Read the final with predicted values
bike <- read.csv(FinalDatafileHourly ,stringsAsFactors = FALSE)
str(bike)
bike$season <- factor(bike$season)
bike$mnth <- factor(bike$mnth)
bike$hr <- factor(bike$hr)
bike$holiday <- factor(bike$holiday)
bike$weekday <- factor(bike$weekday)
bike$workingday <- factor(bike$workingday)
bike$weathersit <- factor(bike$weathersit)
library(RWeka)
### Clustering with Simple Means , 10-clusters
simpleKmeansCLuster10 <- SimpleKMeans(bike[c("temp","atemp","hum","windspeed","workingday","holiday","season","weathersit","casual","registered","cnt","mnth","hr")] ,Weka_control(N=10, V=TRUE))
simpleKmeansCLuster10
summary(simpleKmeansCLuster10)
str(simpleKmeansCLuster10)
```
summary(simpleKmeansCLuster10)
simpleKmeansCLuster10
datBike_w_clusterIds <- cbind(simpleKmeansCLuster10$class_ids,datBike)
datBike_w_clusterIds <- cbind(simpleKmeansCLuster10$class_ids,bike)
summary(datBike_w_clusterIds)
str(datBike_w_clusterIds)
simpleKmeansCLuster10
Cluster_high_casual <- subset(datBike_w_clusterIds, datBike_w_clusterIds$`simpleKmeansCLuster10$class_ids`==2)
Cluster_high_registered <- subset(datBike_w_clusterIds, datBike_w_clusterIds$`simpleKmeansCLuster10$class_ids`==4)
Cluster_low_overall <- subset(datBike_w_clusterIds, datBike_w_clusterIds$`simpleKmeansCLuster10$class_ids`==3)
RpartModel_high_casual <- rpart(casual ~ holiday+workingday+temp +humidity+windspeed+atemp+humidity+time_intervals+season+weather, data = Cluster_high_casual, control=Weka_control(C=0.05))
library(rpart)
RpartModel_high_casual <- rpart(casual ~ holiday+workingday+temp +humidity+windspeed+atemp+humidity+time_intervals+season+weather, data = Cluster_high_casual, control=Weka_control(C=0.05))
RpartModel_high_casual <- rpart(casual ~ holiday+workingday+temp +hum+windspeed+atemp+humidity+time_intervals+season+weather, data = Cluster_high_casual, control=Weka_control(C=0.05))
RpartModel_high_casual <- rpart(casual ~ holiday+workingday+temp +hum+windspeed+atemp+time_intervals+season+weather, data = Cluster_high_casual, control=Weka_control(C=0.05))
RpartModel_high_casual <- rpart(casual ~ holiday+workingday+temp +hum+windspeed+atemp+hr+season+weather, data = Cluster_high_casual, control=Weka_control(C=0.05))
RpartModel_high_casual <- rpart(casual ~ holiday+workingday+temp +hum+windspeed+atemp+hr+season+weathersit, data = Cluster_high_casual, control=Weka_control(C=0.05))
RpartModel_high_casual
# Show the summary of the model built
summary(RpartModel_high_casual)
# Plot the model
rpart.plot(RpartModel_high_casual)
library(rpart.plot)
rpart.plot(RpartModel_high_casual)
RpartModel_high_registered <- rpart(registered ~ holiday+workingday+temp +humidity+windspeed+atemp+hum+hr+season+weathersit, data = Cluster_high_registered, control=Weka_control(C=0.05))
RpartModel_high_registered <- rpart(registered ~ holiday+workingday+temp +windspeed+atemp+hum+hr+season+weathersit, data = Cluster_high_registered, control=Weka_control(C=0.05))
RpartModel_high_registered
# Show the summary of the model built
summary(RpartModel_high_registered)
# Plot the model
rpart.plot(RpartModel_high_registered)
par(mfrow=c(1, 1))
rpart.plot(RpartModel_high_casual)
rpart.plot(RpartModel_high_registered)
RpartModel_overall_low <- rpart(count ~ holiday+workingday+temp +humidity+windspeed+atemp+humidity+time_intervals+season+weather, data = Cluster_low_overall, control=Weka_control(C=0.05))
RpartModel_high_registered
# Show the summary of the model built
summary(RpartModel_overall_low)
# Plot the model
rpart.plot(RpartModel_overall_low)
RpartModel_overall_low <- rpart(count ~ holiday+workingday+temp +hum+windspeed+atemp+hrseason+weathersit, data = Cluster_low_overall, control=Weka_control(C=0.05))
RpartModel_overall_low <- rpart(count ~ holiday+workingday+temp +hum+windspeed+atemp+hr+season+weathersit, data = Cluster_low_overall, control=Weka_control(C=0.05))
RpartModel_overall_low <- rpart(cnt ~ holiday+workingday+temp +hum+windspeed+atemp+hr+season+weathersit, data = Cluster_low_overall, control=Weka_control(C=0.05))
RpartModel_high_registered
# Show the summary of the model built
summary(RpartModel_overall_low)
# Plot the model
rpart.plot(RpartModel_overall_low)
lm1 <- lm(Sales ~ CompPrice + Price + ShelveLoc, data = dt)
summary(lm1)
setwd("E:/MS/PAR/Class7/")
dt <- read.csv("carseats_missing.csv")
str(dt)
```
Question 1
```{r Question1, include=T}
# CompPrice, Price and ShelveLoc
nrow(dt)
lm1 <- lm(Sales ~ CompPrice + Price + ShelveLoc, data = dt)
summary(lm1)
q.dt <- NULL
q.dt$CompPrice <- 125
q.dt$Price <- 115
q.dt$ShelveLoc <- "Medium"
q.dt <- as.data.frame(q.dt)
round(predict(lm1, newdata = q.dt) * 1000,2)
round( (5.026583 + (0.088964*125) + (-0.094195*115) + (2.150290*1) ) * 1000)
set.seed(514)
mimp <- mice(dt)
densityplot(mimp)
# ?with
model1 <- with(data = mimp, exp = lm(Sales ~ CompPrice + Price + ShelveLoc))
summary(pool(model1))
summary(model1$analyses[[i]])
preds <- NULL
for(i in 1:5){
preds <- c(preds,round(predict(model1$analyses[[i]], newdata = q.dt) * 1000,2))
}
mean(preds)
round( (4.65621858 + (0.09001765*125) + (-0.09049272*115) + (2.01294528*1) ) * 1000)
set.seed(514)
model1 <- with(data = mimp, exp = lm(Sales ~ CompPrice + Price + ShelveLoc))
summary(pool(model1))
set.seed(514)
model1 <- with(data = mimp, exp = lm(Sales ~ CompPrice + Price + ShelveLoc))
summary(pool(model1))
round( (4.80666101 + (0.08847354*125) + (-0.09009646*115) + (2.00925986*1) ) * 1000)
round( (4.80666101 + (0.08847354*125) + (-0.09009646*115) + (2.00925986*1) ) * 1000)
mean(preds)
round(mean(preds))
set.seed(514)
mimp <- mice(dt)
densityplot(mimp)
# ?with
set.seed(514)
model1 <- with(data = mimp, exp = lm(Sales ~ CompPrice + Price + ShelveLoc))
summary(pool(model1))
preds <- NULL
for(i in 1:5){
preds <- c(preds,round(predict(model1$analyses[[i]], newdata = q.dt) * 1000,2))
}
round(mean(preds))
round( (4.80666101 + (0.08847354*125) + (-0.09009646*115) + (2.00925986*1) ) * 1000)
dt.car <- read.csv("carseats.csv")
dt.train <- subset(dt.car, group == "train")
dt.test <- subset(dt.car, group == "test")
library(tree)
tm <- tree(Sales ~ ., data = dt.train)
summary(tm)
tm
plot(tm)
text(tm,pretty=0, cex=.75)
str(dt)
set.seed(514)
cvtm <- cv.tree(tm, FUN = prune.tree, K = 10)
cvtm
plot(cvtm)
best <- cvtm$size[which(cvtm$dev==min(cvtm$dev))]
best
prun.model <- prune.tree(tm, best=best)
plot(prun.model)
text(prun.model,pretty=0, cex=.75)
summary(prun.model)
round(rmse(dt.test$Sales, predict(prun.model,newdata = dt.test))*1000,2)
rmse <- function(y, yhat) {
sqrt(mean((y - yhat)^2))
}
round(rmse(dt.test$Sales, predict(prun.model,newdata = dt.test))*1000,2)
set.seed(514)
crf <- train(Sales~., method = "rf", data = dt.train)
crf
round(rmse(dt.test$Sales, predict(crf,newdata = dt.test))*1000,2)
plot(cvtm)
best
prun.model <- prune.tree(tm, best=best)
plot(prun.model)
text(prun.model,pretty=0, cex=.75)
summary(prun.model)
predict(prun.model,newdata = dt.test)
set.seed(514)
crf <- train(Sales~., method = "rf", data = dt.train)
round(rmse(dt.test$Sales, predict(crf,newdata = dt.test))*1000,2)
setwd("E:/MS/PAR/Class7/")
dt <- read.csv("carseats.csv")
str(dt)
table(dt$group)
train <- dt[dt$group == "train",]
str(train)
train$group <- NULL
test <- dt[dt$group == "test",]
str(test)
test$group <-NULL
library(tree)
model_tree <- tree(Sales ~ ., data = train)
summary(model_tree)
plot(model_tree)
set.seed(514)
cvm <- cv.tree(model_tree, FUN = prune.tree, K = 10)
library(caret)
model_rf <- train(Sales~., method = "rf", data = train)
round(rmse(test$Sales*1000, predict(model_rf, newdata = test)*1000),2)
rmse <- function(actual, pred) sqrt(mean((actual-pred)^2))
round(rmse(test$Sales*1000, predict(model_rf, newdata = test)*1000),2)
set.seed(514)
model_rf <- train(Sales~., method = "rf", data = train)
round(rmse(test$Sales*1000, predict(model_rf, newdata = test)*1000),2)
setwd("E:/MS/PAR/GroupProject")
dt.train <- read.csv('train.csv', stringsAsFactors = FALSE)
dt.test <- read.csv('test.csv')
dim(dt.train)
str(dt.train)
```
Certain columns have missing values(NAs). Below is the summary of all missing value information.
```{r Summarizing information on missing values, include=T,echo=F}
dt.train <- read.csv('train.csv', stringsAsFactors = FALSE)
dt.test <- read.csv('test.csv')
dim(dt.train)
str(dt.train)
mis.vars <-data.frame(colSums(sapply(dt.train, is.na)))
colnames(mis.vars) <- c('No_of_NAs')
kable(subset(mis.vars, No_of_NAs > 0 ))
library(FSelector)
library(mlbench)
library(ggplot2)
library(glmnet)
library(lars)
library(RColorBrewer)
library(reshape2)
library(ggplot2)
library(e1071)
library(dplyr)
library(Amelia)
library(RANN)
library(arm)
library(caret)
library(ipred)
library(corrplot)
library(knitr)
mean_sp <- mean(dt.train$SalePrice)
kable(subset(mis.vars, No_of_NAs > 0 ))
mean_sp <- mean(dt.train$SalePrice)
median_sp <- median(dt.train$SalePrice)
sd_sp <- sd(dt.train$SalePrice)
sale_price_stats <- data.frame(mean_sp, median_sp, sd_sp)
sale_price <- data.frame(dt.train$SalePrice)
sale_price_stats
hist(sale_price$dt.train.SalePrice,xlim = c(1000,800000),main = "Sale Price Distribution",xlab = 'Sale Price',freq = FALSE,col=brewer.pal(8,"Set3"),las = 3,breaks = 190)
lines(density(sale_price$dt.train.SalePrice))
hist(log(sale_price$dt.train.SalePrice),main = "Log of Sale Price Distribution",xlab = 'Sale Price',freq = FALSE,col=brewer.pal(8,"Set3"),las = 3,breaks = 190)
lines(density(log(sale_price$dt.train.SalePrice)))
```
```{r, include=F}
numeric_var <- names(dt.train)[which(sapply(dt.train, is.numeric))]
df.corr <- data.frame(cor(dt.train[,(numeric_var)], method="pearson"))
#View(df.corr)
typeof(df.corr)
#Correlation with Each variables and Sale Price:
df.sale.corr <- data.frame(abs(df.corr[,38]))
df.sale.corr$features <- names(df.corr)
#View(df.sale.corr)
df.sale.corr<- df.sale.corr[order(-df.sale.corr$abs.df.corr...38..),]
#View(df.sale.corr) #Ordered list of Correlations
#Top 5 correlated features
top5Corr <- df.sale.corr[2:7,]
```
library(ISLR)
data(Carseats)
c <- Carseats
#produce a dataset with missing observations
#install.packages("missForest")
library(missForest)
install.packages("missForest")
library(missForest)
cmissx <- prodNA(c[,-1], noNA=.25)
cmiss <- cbind(Sales=c[,1], cmissx)
head(cmiss)
sum(complete.cases(cmiss))/nrow(cmiss)
#The caret imputation functions work for continuous variables but not
#categorical variables:
library(caret)
str(cmiss)
knnimp <- predict(preProcess(cmiss, method=c("knnImpute")), cmiss)
summary(knnimp)
bagimp <- predict(preProcess(cmiss, method=c("bagImpute")), cmiss)
medimp <- predict(preProcess(cmiss, method=c("medianImpute")), cmiss)
summary(medimp)
complete.cases(cmiss)
sum(complete.cases(cmiss))
sum(complete.cases(cmiss))/nrow(cmiss)
library(missForest)
cmissx <- prodNA(c[,-1], noNA=.25)
cmiss <- cbind(Sales=c[,1], cmissx)
head(cmiss)
sum(complete.cases(cmiss))/nrow(cmiss)
str(cmiss)
knnimp <- predict(preProcess(cmiss, method=c("knnImpute")), cmiss)
summary(knnimp)
bagimp <- predict(preProcess(cmiss, method=c("bagImpute")), cmiss)
medimp <- predict(preProcess(cmiss, method=c("medianImpute")), cmiss)
summary(medimp)
#knnImpute leaves NAs in the categorical variables, as does medianImpute,
#and bagImpute fails.
#You can impute in the context of train using the preProcess option,
# if you also specify na.action = na.pass:
med <- summary(train(Sales ~ .,
data = cmiss,
preProcess = c("center","scale","medianImpute"),
na.action=na.pass,
method = "lm"))
knn <- summary(train(Sales ~ .,
data = cmiss,
preProcess = c("center","scale","knnImpute"),
na.action=na.pass,
method = "lm"))
bag <- summary(train(Sales ~ .,
data = cmiss,
preProcess = c("center","scale","bagImpute"),
na.action=na.pass,
method = "lm"))
method = "lm"))
method = "lm"))
med <- summary(train(Sales ~ .,
data = cmiss,
preProcess = c("center","scale","medianImpute"),
na.action=na.pass,
method = "lm"))
knn <- summary(train(Sales ~ .,
data = cmiss,
preProcess = c("center","scale","knnImpute"),
na.action=na.pass,
method = "lm"))
knn <- summary(train(Sales ~ .,
data = cmiss,
preProcess = c("center","scale","knnImpute"),
na.action=na.pass,
method = "lm"))
bag <- summary(train(Sales ~ .,
data = cmiss,
preProcess = c("center","scale","bagImpute"),
na.action=na.pass,
method = "lm"))
original <- summary(train(Sales ~ .,
data = c,
preProcess = c("center","scale"),
method = "lm"))
nothing <- summary(train(Sales ~ .,
data = cmiss,
preProcess = c("center","scale"),
na.action=na.pass,
method = "lm"))
mod_df <- rbind(data.frame(variables = row.names(coef(med)),
coefs = coef(med)[,1],
method = "median"),
data.frame(variables  = row.names(coef(knn)),
coefs = coef(knn)[,1],
method = "knn"),
data.frame(variables  = row.names(coef(bag)),
coefs =coef(bag)[,1],
method = "bag"),
data.frame(variables  = row.names(coef(nothing)),
coefs = coef(nothing)[,1],
method = "nothing"),
data.frame(variables  = row.names(coef(original)),
coefs = coef(original)[,1],
method = "original"))
mod_df
mfimp <- missForest(cmiss)
missF <- summary(train(Sales ~ .,
data = mfimp$ximp,
preProcess = c("center","scale"),
method = "lm"))
mod_df <- rbind(mod_df, data.frame(variables = row.names(coef(missF)),
coefs =coef(med)[,1],
method= "missF"))
ggplot(mod_df, aes(variables, coefs, col= method, group=method)) +
geom_line() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ggtitle("Comparison of imputation methods")
m <- lm(Sales ~ Price + ShelveLoc + CompPrice, data = c)
summary(m)
install.packages("ranger")
rf <- train(Sales ~ .,
data= c,
preProcess=c("center","scale"),
method="ranger")
rf
rf
plot(rf)
varImp(rf)
varImp(rf, useModel = F)
#change tuneLength
rf <- train(Sales ~ .,
data= c,
tuneLength=10,
preProcess=c("center","scale"),
method="ranger")
plot(rf)
lm <- train(Sales ~ .,
data= c,
preProcess=c("center","scale"),
method="lm")
lm
varImp(lm)
plot(lm)
#Boosted stump model
gbm <- train(Sales ~ .,
data= c,
preProcess=c("center","scale"),
method="gbm")
gbm
plot(gbm)
varImp(gbm)
library(MASS)
data("Pima.te")
data("Pima.tr")
p <- rbind(Pima.te, Pima.tr)
names(p)
train(type ~.,
method="glm",
preProcess=c("center","scale"),
data=p)
train(type ~.,
method="gbm",
preProcess=c("center","scale"),
data=p,
verbose=F)
train(type ~.,
method="xgbTree",
preProcess=c("center","scale"),
data=p)
train(type ~.,
method="ranger",
preProcess=c("center","scale"),
data=p)
dt <- Boston
dt <- Boston
data=dt)
train(type ~.,
method="glm",
preProcess=c("center","scale"),
data=dt)
str(dt)
train(medv ~.,
method="glm",
preProcess=c("center","scale"),
data=dt)
train(medv ~.,
method="gbm",
preProcess=c("center","scale"),
data=dt,
verbose=F)
train(medv ~.,
method="ranger",
preProcess=c("center","scale"),
data=dt)
train(medv ~.,
method="xgbTree",
preProcess=c("center","scale"),
data=dt)
