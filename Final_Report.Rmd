---
title: "Final_Report"
author: "Sai Deepthi Matam, Sri Harsha Samanthula" 
output: pdf_document
---



### OBJECTIVE

  Initial focus of the project is to gain knowledge of the data and understand the relation between each of the variables to the house's sale price. Later, further statistical analysis will be conducted to select any 5 variables which tend to effect the price the most.
  
  Later, using the training data set, a best-fitting model will be constructed with the 5 variables as predictors of housing prices. Performance of various statistical models will be compared against each other to determine which model fits the best.

### ABOUT THE DATA

  The data set available on Kaggle contains 80 variables that involve in assessing home values. Out of these, 20 are continuous, 14 are discrete and the remaining 46 are categorical variables. This data has been randomized and then split in to two sets(train and test) of equal size. "SalePrice" is the outcome variable
  
```{r ,include=F}
### REQUIRED PACKAGES
# install.packages("e1071")
# install.packages("Amelia")
# install.packages("RANN")
# install.packages("ipred")
# install.packages("corrplot")
# install.packages("RColorBrewer")
# install.packages("lars")
# install.packages("glmnet")
# install.packages("ggplot2")
# install.packages("devtools")
# install_github("easyGgplot2", "kassambara")
# install.packages("FSelector")
# install.packages("mlbench")

library(FSelector)
library(mlbench)
# library(easyGgplot2)
# library(devtools)
library(ggplot2)
library(glmnet)
library(lars)
library(RColorBrewer)
library(reshape2)
library(ggplot2)
library(e1071)
library(dplyr)
library(Amelia)
library(RANN)
library(arm)
library(caret)
library(ipred)
library(corrplot)
library(knitr)
library(caretEnsemble)

```


```{r, include=F}
### DEFINING USEFUL FUNCTIONS
rmse <- function(y, yhat) {
  sqrt(mean((y - yhat)^2))
}

rmsle <- function(a, p) { 
   return(sqrt(1/length(a)*sum((log(p +1)-log(a +1))^2)))
}

stats <- function(x){
    c( 
       N = sum(x, na.rm = T) ,
       MEAN = mean(x) , 
       SD = sd(x, na.rm = FALSE) , 
       SEM = sd(x)/sqrt(length(x)) ,
       LOWER_CI = mean(x) - (1.96 * (sd(x)/sqrt(length(x)))) ,
       UPPER_CI = mean(x) + (1.96 * (sd(x)/sqrt(length(x)))) ,
       L = length(x)
     )
}

```


```{r Loading the data, include=F}

# setwd('/Users/dmatam/Google Drive/1_PC/Predictive Analytics with R/Final Project/Raw Data/')
#setwd("E:/MS/PAR/GroupProject/PAR-GP")
setwd("/Users/mdobbali/Google Drive/University/Projects/PAR-GP/")

dt.train <- read.csv('train.csv', stringsAsFactors = FALSE)
dt.test <- read.csv('test.csv', stringsAsFactors = FALSE)
dim(dt.train)
str(dt.train)


```

Certain columns have missing values(NAs). Below is the summary of all missing value information.

Train Set ::
```{r Summarizing information on missing values in train set, include=T,echo=F}

numCols.tr <- NULL
charCols.tr <- NULL
misc.tr <- NULL
for (i in 1:ncol(dt.train)){
  if(is.numeric(dt.train[,i])){
    numCols.tr <- c(numCols.tr,names(dt.train)[i])
  }
  else if(is.character(dt.train[,i])){
    charCols.tr <- c(charCols.tr,names(dt.train)[i])
  }
  else {
    misc.tr <- c(misc.tr,names(dt.train)[i])
  }
}

mis.vars <-data.frame(colSums(sapply(dt.train, is.na)))
mis.vars$num_yn <- "Z"
for(i in 1:nrow(mis.vars)){
  if( rownames(mis.vars)[i] %in% numCols.tr){
    mis.vars[i,2] <- "Y"
  }else{
    mis.vars[i,2] <- "N"
  }
}

colnames(mis.vars) <- c('No_of_NAs', 'Numerical(y/n)')
kable(subset(mis.vars, No_of_NAs > 0 ))

```
Test Set ::
```{r Summarizing information on missing values in test set, include=T,echo=F}

numCols.te <- NULL
charCols.te <- NULL
misc.te <- NULL
for (i in 1:ncol(dt.test)){
  if(is.numeric(dt.test[,i])){
    numCols.te <- c(numCols.te,names(dt.test)[i])
  }
  else if(is.character(dt.test[,i])){
    charCols.te <- c(charCols.te,names(dt.test)[i])
  }
  else {
    misc.te <- c(misc.te,names(dt.test)[i])
  }
}

mis.vars.te <-data.frame(colSums(sapply(dt.test, is.na)))
mis.vars.te$num_yn <- "Z"
for(i in 1:nrow(mis.vars.te)){
  if( rownames(mis.vars.te)[i] %in% numCols.te){
    mis.vars.te[i,2] <- "Y"
  }else{
    mis.vars.te[i,2] <- "N"
  }
}

colnames(mis.vars.te) <- c('No_of_NAs', 'Numerical(y/n)')
kable(subset(mis.vars.te, No_of_NAs > 0 ))

```

Here, out of 80 varaibles, there are only 3 variables that has missing values. Single imputations works well in this case. So, we used Bagimpute 

NAs in character variables: All character variables contain the category of a certain feature available in the house. As per the data description from Kaggle, NAs in such cases means absence of that feature. Hence, replacing NAs with more descriptive words.

```{r Performing Imputation on numeric variables2, include=F}

bagImpute <- predict(preProcess(dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))], method = c("bagImpute")), dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))])
bagImpute

dt.train$GarageYrBlt <- round(bagImpute$GarageYrBlt)
dt.train$MasVnrArea <- bagImpute$MasVnrArea
dt.train$LotFrontage <- bagImpute$LotFrontage

bagImpute <- predict(preProcess(dt.test[,which(names(dt.test) %in% rownames(subset(mis.vars.te, (mis.vars.te$No_of_NAs>0 & mis.vars.te[,2] == "Y"))))], method = c("bagImpute")), dt.test[,which(names(dt.test) %in% rownames(subset(mis.vars.te, (mis.vars.te$No_of_NAs>0 & mis.vars.te[,2] == "Y"))))])
bagImpute

dt.test$GarageYrBlt <- round(bagImpute$GarageYrBlt)
dt.test$MasVnrArea <- bagImpute$MasVnrArea
dt.test$LotFrontage <- bagImpute$LotFrontage
dt.test$BsmtFinSF1 <- bagImpute$BsmtFinSF1
dt.test$BsmtFinSF2 <- bagImpute$BsmtFinSF2
dt.test$BsmtUnfSF <- bagImpute$BsmtUnfSF
dt.test$TotalBsmtSF <- bagImpute$TotalBsmtSF
dt.test$BsmtFullBath <- bagImpute$BsmtFullBath
dt.test$GarageCars <- round(bagImpute$GarageCars)
dt.test$GarageArea <- bagImpute$GarageArea

```


```{r Handling NAs in character variables, include=F}

dt.train$Alley <- ifelse(is.na(dt.train$Alley),"No Alley", dt.train$Alley)

dt.train$MasVnrType <- ifelse(is.na(dt.train$MasVnrType),"None", dt.train$MasVnrType)
# some data has masvnrtype none and area <> 0
#subset(dt.train, dt.train$MasVnrType == "None")$MasVnrArea 
dt.train$Electrical<- ifelse(is.na(dt.train$Electrical),"SBrKr",dt.train$Electrical)

dt.train$BsmtQual <- ifelse(is.na(dt.train$BsmtQual),"NoBsmt", dt.train$BsmtQual)

dt.train$BsmtCond <- ifelse(is.na(dt.train$BsmtCond),"NoBsmt", dt.train$BsmtCond)

dt.train$BsmtExposure <- ifelse(is.na(dt.train$BsmtExposure),"NoBsmt", dt.train$BsmtExposure)

dt.train$BsmtFinType1 <- ifelse(is.na(dt.train$BsmtFinType1),"NoBsmt", dt.train$BsmtFinType1)

dt.train$BsmtFinType2 <- ifelse(is.na(dt.train$BsmtFinType2),"NoBsmt", dt.train$BsmtFinType2)

dt.train$FireplaceQu <- ifelse(is.na(dt.train$FireplaceQu),"NoFireplace", dt.train$FireplaceQu)

dt.train$GarageType <- ifelse(is.na(dt.train$GarageType),"NoGarage", dt.train$GarageType)

dt.train$GarageFinish <- ifelse(is.na(dt.train$GarageFinish),"NoGarage", dt.train$GarageFinish)

dt.train$GarageQual <- ifelse(is.na(dt.train$GarageQual),"NoGarage", dt.train$GarageQual)

dt.train$GarageCond <- ifelse(is.na(dt.train$GarageCond),"NoGarage", dt.train$GarageCond)

dt.train$GarageCond <- ifelse(is.na(dt.train$GarageCond),"NoGarage", dt.train$GarageCond)

dt.train$PoolQC <- ifelse(is.na(dt.train$PoolQC),"NoPool", dt.train$PoolQC)

dt.train$Fence <- ifelse(is.na(dt.train$Fence),"NoFence", dt.train$Fence)

dt.train$MiscFeature <- ifelse(is.na(dt.train$MiscFeature),"NoFence", dt.train$MiscFeature)


dt.test$Alley <- ifelse(is.na(dt.test$Alley),"No Alley", dt.test$Alley)

dt.test$MasVnrType <- ifelse(is.na(dt.test$MasVnrType),"None", dt.test$MasVnrType)
# some data has masvnrtype none and area <> 0
#subset(dt.test, dt.test$MasVnrType == "None")$MasVnrArea 

dt.test$BsmtQual <- ifelse(is.na(dt.test$BsmtQual),"NoBsmt", dt.test$BsmtQual)

dt.test$BsmtCond <- ifelse(is.na(dt.test$BsmtCond),"NoBsmt", dt.test$BsmtCond)

dt.test$BsmtExposure <- ifelse(is.na(dt.test$BsmtExposure),"NoBsmt", dt.test$BsmtExposure)

dt.test$BsmtFinType1 <- ifelse(is.na(dt.test$BsmtFinType1),"NoBsmt", dt.test$BsmtFinType1)

dt.test$BsmtFinType2 <- ifelse(is.na(dt.test$BsmtFinType2),"NoBsmt", dt.test$BsmtFinType2)

dt.test$FireplaceQu <- ifelse(is.na(dt.test$FireplaceQu),"NoFireplace", dt.test$FireplaceQu)

dt.test$GarageType <- ifelse(is.na(dt.test$GarageType),"NoGarage", dt.test$GarageType)

dt.test$GarageFinish <- ifelse(is.na(dt.test$GarageFinish),"NoGarage", dt.test$GarageFinish)

dt.test$GarageQual <- ifelse(is.na(dt.test$GarageQual),"NoGarage", dt.test$GarageQual)

dt.test$GarageCond <- ifelse(is.na(dt.test$GarageCond),"NoGarage", dt.test$GarageCond)

dt.test$GarageCond <- ifelse(is.na(dt.test$GarageCond),"NoGarage", dt.test$GarageCond)

dt.test$PoolQC <- ifelse(is.na(dt.test$PoolQC),"NoPool", dt.test$PoolQC)

dt.test$Fence <- ifelse(is.na(dt.test$Fence),"NoFence", dt.test$Fence)

dt.test$MiscFeature <- ifelse(is.na(dt.test$MiscFeature),"NoFence", dt.test$MiscFeature)

dt.test$MSZoning <- ifelse(is.na(dt.test$MSZoning),"RM",dt.test$MSZoning)

dt.test$Utilities <- ifelse(is.na(dt.test$Utilities),"AllPub",dt.test$Utilities)

dt.test$Exterior1st <- ifelse(is.na(dt.test$Exterior1st),"WdShing",dt.test$Exterior1st)

dt.test$Exterior2nd <- ifelse(is.na(dt.test$Exterior2nd),"Stone", dt.test$Exterior2nd)

dt.test$BsmtHalfBath <- ifelse(is.na(dt.test$BsmtHalfBath),0,dt.test$BsmtHalfBath)

dt.test$KitchenQual <- ifelse(is.na(dt.test$KitchenQual),"TA",dt.test$KitchenQual)

dt.test$Functional <- ifelse(is.na(dt.test$Functional),"Typ",dt.test$Functional)

dt.test$SaleType <- ifelse(is.na(dt.test$SaleType),"WD",dt.test$SaleType)

char_var <- names(dt.train)[which(sapply(dt.train, is.character))]
for(name in char_var){
  #print(name)
  dt.train[[name]] <- factor(dt.train[[name]])
}

char_var <- names(dt.test)[which(sapply(dt.test, is.character))]
for(name in char_var){
  #print(name)
  dt.test[[name]] <- factor(dt.test[[name]])
}


```

Numerical Variables, 
Checking for collinearity.

```{r Checking , include=F}

numeric_var <- names(dt.train)[which(sapply(dt.train, is.numeric))]
df.corr <- data.frame(cor(dt.train[,(numeric_var)], method="pearson"))

#Correlation with Each variables and Sale Price:
df.sale.corr <- data.frame(abs(df.corr[,38]))
df.sale.corr$features <- names(df.corr)
#View(df.sale.corr)

df.sale.corr<- df.sale.corr[order(-df.sale.corr$abs.df.corr...38..),]
#View(df.sale.corr) #Ordered list of Correlations

#Top 5 correlated features
top5Corr <- df.sale.corr[2:7,]

```

Top 5 Correlation Numerical Variables

```{r , include=T,echo=F}

colnames(top5Corr)<- c("Cors","Features")
row.names(top5Corr) <- NULL

kable(top5Corr[,c(2,1)])

```


```{r , include=T,echo=F}
correlations <- cor(dt.train[, numeric_var], use = "everything")
corrplot(correlations, method = "circle", type="lower",  sig.level = 0.01, insig = "blank", tl.col = "grey",tl.cex = 0.6)
```

```{r inspecting multicolinearity, include=F}

df.corr <- data.frame(cor(dt.train[,(numeric_var)], method="pearson"))

df.mul.cor <- NULL 
for(i in 1:nrow(df.corr)){
  for(j in 1:i){
    df.temp <- NULL
    if(!is.na(df.corr[i,j])){
      if(df.corr[i,j] >= 0.6 && df.corr[i,j] != 1){
        df.temp$name1 <- names(df.corr)[i]
        df.temp$name2 <- names(df.corr)[j]
        df.temp$cor <- df.corr[i,j]
        df.mul.cor <- rbind(df.mul.cor,df.temp)
      }
    }
  }
}
df.mul.cor <- as.data.frame(df.mul.cor)

```
Following table contains the combinations of variables with highest correlation which has a minimum of 0.6 as corelation value. This will identify redundant predictors 

```{r, include=T, echo=F}

df.mul.cor <- subset(df.mul.cor, name1 != "SalePrice")
kable(df.mul.cor, row.names=FALSE)

```

Combining Bath into one variable 
BsmtFullbath, BsmtHalfBath, FullBath, HalfBath

```{r, include=T, echo=F}
dt.train$bathrooms <- dt.train$BsmtFullBath+ (0.5  * dt.train$BsmtHalfBath) +dt.train$FullBath+ (0.5*dt.train$HalfBath)

dt.train$porch <- dt.train$WoodDeckSF + dt.train$OpenPorchSF + dt.train$EnclosedPorch + dt.train$X3SsnPorch + dt.train$ScreenPorch

dt.train$totalRoom <- dt.train$TotRmsAbvGrd + dt.train$KitchenAbvGr


dt.test$bathrooms <- dt.test$BsmtFullBath+ (0.5  * dt.test$BsmtHalfBath) +dt.test$FullBath+ (0.5*dt.test$HalfBath)

dt.test$porch <- dt.test$WoodDeckSF + dt.test$OpenPorchSF + dt.test$EnclosedPorch + dt.test$X3SsnPorch + dt.test$ScreenPorch

dt.test$totalRoom <- dt.test$TotRmsAbvGrd + dt.test$KitchenAbvGr

```


can be removed.
```{r}


lasso_mod <- train(frmla, data = dt.train,
                   preProcess = c("center", "scale"),
                   method = "glmnet",
                   tuneGrid= expand.grid(
                     alpha=1,
                     lambda = 0:20/20))

#plot(lasso_mod)
plot(lasso_mod$finalModel)
legend(num_coef$variables)

lasso_mod$finalModel$tuneValue
num_coef <- data.frame(as.matrix(coef(lasso_mod$finalModel, lasso_mod$finalModel$tuneValue$lambda)))
num_coef$variables <- rownames(num_coef)
num_coef$coeffiecient <- num_coef$X1
rownames(num_coef) <- NULL
num_coef$X1 <- NULL
num_coef[order(-abs(num_coef$coeffiecient)),]

rmse(dt.train$SalePrice,predict(lasso_mod, newdata = dt.train))

RMSLE <- function(A, P) {
    return (sqrt(sum((log(P+1)-log(A+1))^2)/length(A)))
}
RMSLE(dt.train$SalePrice,predict(lasso_mod, newdata = dt.train))

ridge_mod <- train(frmla, 
                data = dt.train,
                preProcess = c("center", "scale"),
                method = "glmnet",
                tuneGrid= expand.grid(
                  alpha=0,
                  lambda = 0:10))
#plot(ridge_mod)

lm_mod <- train(frmla, 
                data = dt.train,
               preProcess = c("center", "scale"),
                method = "lm")

compare_temp  <- data.frame(variables = as.character(names(coef(lm_mod$finalModel))))

compare_temp$method <- rep("lasso")
compare_temp$coefs <- c(as.numeric(as.character(coef(lasso_mod$finalModel, 
                                                     lasso_mod$finalModel$tuneValue$lambda))))

compare <- data.frame(variables = rep(as.character(names(coef(lm_mod$finalModel))),2))
compare$method <- c(rep("ridge"), rep("lm"))
compare$coefs <- c(as.numeric(as.character(coef(ridge_mod$finalModel, ridge_mod$bestTune$lambda))),
                   as.numeric(as.character(coef(lm_mod$finalModel))))

compare <- rbind(compare, compare_temp)

ggplot(compare, aes(variables, coefs, group=method, fill=method)) + 
  geom_bar(stat = "identity",position="dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```

## Feature Selection

```{r}

dt.train1 <- dt.train
dt.test1 <- dt.test

dt.train$SalePriceL <- log(dt.train$SalePrice+1)

hist(dt.train$SalePrice)
hist(dt.train$SalePriceL)

dt.train$SalePrice <- dt.train$SalePriceL
dt.train$SalePriceL <- NULL

# train set
skewed_feats <- sapply(numCols.tr,function(x){skewness(dt.train[[x]],na.rm=TRUE)})

skewed_feats <- skewed_feats[skewed_feats > 0.75]

for(x in names(skewed_feats)) {
  dt.train[[x]] <- log(dt.train[[x]] + 1)
}

# test set
skewed_feats <- sapply(numCols.te,function(x){skewness(dt.test[[x]],na.rm=TRUE)})

skewed_feats <- skewed_feats[skewed_feats > 0.75]
for(x in names(skewed_feats)) {
  dt.test[[x]] <- log(dt.test[[x]] + 1)
}


X_train <- dt.train
X_train$Id <- NULL
X_test <- dt.test
y <- dt.train$SalePrice

lambdas <- seq(1,0,-0.001)

train.cr <- trainControl(method="repeatedcv",
                                 number=5,
                                 repeats=5,
                                 verboseIter=FALSE)
set.seed(123)

model_ridge <- train(SalePrice~., 
                data = X_train,
                  method="glmnet",
                  metric="RMSE",
                preProcess = c("center", "scale"),
                  maximize=FALSE,
                  trControl=train.cr,
                  tuneGrid=expand.grid(alpha=0, # Ridge regression
                                       lambda=lambdas))

ggplot(data=filter(model_ridge$result,RMSE<0.14)) +
    geom_line(aes(x=lambda,y=RMSE))

set.seed(123)  # for reproducibility
model_lasso <- train(SalePrice~., 
                data = X_train,
                  method="glmnet",
                preProcess = c("center", "scale"),
                  metric="RMSE",
                  maximize=FALSE,
                  trControl=train.cr,
                  tuneGrid=expand.grid(alpha=1,  # Lasso regression
                                       lambda=c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001),
                                            0.00075,0.0005,0.0001)))
model_lasso

coef <- data.frame(coef.name = dimnames(coef(model_lasso$finalModel,s=model_lasso$bestTune$lambda))[[1]], 
           coef.value = matrix(coef(model_lasso$finalModel,s=model_lasso$bestTune$lambda)))

# exclude the (Intercept) term
coef <- coef[-1,]

picked_features <- nrow(filter(coef,coef.value!=0))
not_picked_features <- nrow(filter(coef,coef.value==0))

cat("Lasso picked",picked_features,"variables and eliminated the other",
    not_picked_features,"variables\n")

# sort coefficients in ascending order
coef <- arrange(coef,-coef.value)

# extract the top 10 and bottom 10 features
imp_coef <- rbind(head(coef,15),
                  tail(coef,15))


ggplot(imp_coef) +
    geom_bar(aes(x=reorder(coef.name,coef.value),y=coef.value),
             stat="identity") +
    ylim(-0.3,0.3) +
    coord_flip() +
    ggtitle("Coefficents in the Lasso Model") +
    theme(axis.title=element_blank())

coef

imp_coef$coef.name

dt.train$Conditio

```


GrLivArea           OverallQual         YearBuilt           LotArea            
OverallCond         GarageCars          bathrooms           BsmtFinSF1          
Neighborhood  Functional       ExterQual        HeatingQC         PoolQC           
KitchenAbvGr        Condition2  

Creating model ensemble 

```{r}

index <- sample(1:(0.75*nrow(dt.train)), replace = FALSE)

t.train <- dt.train[index,]
nrow(t.train)
t.test <- dt.train[-index,]
nrow(t.test)

```


```{r}

plot(dt.train$bathrooms, dt.train$SalePrice)

frmla = formula("SalePrice ~ GrLivArea + OverallQual + YearBuilt + LotArea+GarageCars+bathrooms+Neighborhood + TotalBsmtSF")

lm.standard <- train(frmla, preProcess=c("center","scale"), data = t.train, method="lm")

coefplot(lm.standard$finalModel)

rmse(exp(t.test$SalePrice)-1,exp(predict(lm.standard, newdata = t.test))-1)

rmse(exp(t.train$SalePrice)-1,exp(predict(lm.standard, newdata = t.train))-1)


xgBoostm <- train(frmla, preProcess=c("center","scale"), data = t.train, method="xgbTree")

rmse(exp(t.test$SalePrice)-1,exp(predict(lm.standard, newdata = t.test))-1)

rmse(exp(t.train$SalePrice)-1,exp(predict(lm.standard, newdata = t.train))-1)



gbm_mod <- train(frmla, preProcess=c("center","scale"), data = t.train, method="gbm", metric="RMSE")

rmse(exp(t.test$SalePrice)-1,exp(predict(gbm_mod, newdata = t.test))-1)

rmse(exp(t.train$SalePrice)-1,exp(predict(gbm_mod, newdata = t.train))-1)




model_list <- caretList(frmla, 
                        data=dt.train,
                        trControl= trainControl(savePredictions = "final"),
                        preProcess=c("center","scale"),
                        methodList=c("glmnet","gbm", "xgbTree", "ranger")
                        )

summary(resamples(model_list))
modelCor(resamples(model_list))

ce <- caretEnsemble(model_list)
summary(ce)

rmse(exp(t.test$SalePrice)-1,exp(predict(ce, newdata = t.test))-1)

rmse(exp(t.train$SalePrice)-1,exp(predict(ce, newdata = t.train))-1)

result <- exp(predict(ce, newdata = dt.test))-1

dt.result <- NULL
dt.result$Id <- dt.test$Id
dt.result$SalePrice <- result

str(dt.test)
str(result)

dt.result <- as.data.frame(dt.result, row.names = NULL)

write.csv(dt.result, 'Result.csv')
```
