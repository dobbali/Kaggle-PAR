---
title: "Final_Report"
author: "Sai Deepthi Matam, Sri Harsha Samanthula" 
output: pdf_document
---



### OBJECTIVE

  Initial focus of the project is to gain knowledge of the data and understand the relation between each of the variables to the house's sale price. Later, further statistical analysis will be conducted to select any 5 variables which tend to effect the price the most.
  
  Later, using the training data set, a best-fitting model will be constructed with the 5 variables as predictors of housing prices. Performance of various statistical models will be compared against each other to determine which model fits the best.

### ABOUT THE DATA

  The data set available on Kaggle contains 80 variables that involve in assessing home values. Out of these, 20 are continuous, 14 are discrete and the remaining 46 are categorical variables. This data has been randomized and then split in to two sets(train and test) of equal size. "SalePrice" is the outcome variable
  
```{r ,include=F}
### REQUIRED PACKAGES
# install.packages("e1071")
# install.packages("Amelia")
# install.packages("RANN")
# install.packages("ipred")
# install.packages("corrplot")
# install.packages("RColorBrewer")
# install.packages("lars")
# install.packages("glmnet")
# install.packages("ggplot2")
# install.packages("devtools")
# install_github("easyGgplot2", "kassambara")
# install.packages("FSelector")
# install.packages("mlbench")

library(FSelector)
library(mlbench)
# library(easyGgplot2)
# library(devtools)
library(ggplot2)
library(glmnet)
library(lars)
library(RColorBrewer)
library(reshape2)
library(ggplot2)
library(e1071)
library(dplyr)
library(Amelia)
library(RANN)
library(arm)
library(caret)
library(ipred)
library(corrplot)
library(knitr)
library(caretEnsemble)

```


```{r, include=F}
### DEFINING USEFUL FUNCTIONS
rmse <- function(y, yhat) {
  sqrt(mean((y - yhat)^2))
}

rmsle <- function(a, p) { 
   return(sqrt(1/length(a)*sum((log(p +1)-log(a +1))^2)))
}

stats <- function(x){
    c( 
       N = sum(x, na.rm = T) ,
       MEAN = mean(x) , 
       SD = sd(x, na.rm = FALSE) , 
       SEM = sd(x)/sqrt(length(x)) ,
       LOWER_CI = mean(x) - (1.96 * (sd(x)/sqrt(length(x)))) ,
       UPPER_CI = mean(x) + (1.96 * (sd(x)/sqrt(length(x)))) ,
       L = length(x)
     )
}

```


```{r Loading the data, include=F}

# setwd('/Users/dmatam/Google Drive/1_PC/Predictive Analytics with R/Final Project/Raw Data/')
#setwd("E:/MS/PAR/GroupProject/PAR-GP")
setwd("/Users/mdobbali/Google Drive/University/Projects/PAR-GP/")

dt.train <- read.csv('train.csv', stringsAsFactors = FALSE)
dt.test <- read.csv('test.csv', stringsAsFactors = FALSE)
dim(dt.train)
str(dt.train)


```

Certain columns have missing values(NAs). Below is the summary of all missing value information.

Train Set ::
```{r Summarizing information on missing values in train set, include=T,echo=F}

numCols.tr <- NULL
charCols.tr <- NULL
misc.tr <- NULL
for (i in 1:ncol(dt.train)){
  if(is.numeric(dt.train[,i])){
    numCols.tr <- c(numCols.tr,names(dt.train)[i])
  }
  else if(is.character(dt.train[,i])){
    charCols.tr <- c(charCols.tr,names(dt.train)[i])
  }
  else {
    misc.tr <- c(misc.tr,names(dt.train)[i])
  }
}

mis.vars <-data.frame(colSums(sapply(dt.train, is.na)))
mis.vars$num_yn <- "Z"
for(i in 1:nrow(mis.vars)){
  if( rownames(mis.vars)[i] %in% numCols.tr){
    mis.vars[i,2] <- "Y"
  }else{
    mis.vars[i,2] <- "N"
  }
}

colnames(mis.vars) <- c('No_of_NAs')
kable(subset(mis.vars, No_of_NAs > 0 ))

```
Test Set ::
```{r Summarizing information on missing values in test set, include=T,echo=F}

numCols.te <- NULL
charCols.te <- NULL
misc.te <- NULL
for (i in 1:ncol(dt.test)){
  if(is.numeric(dt.test[,i])){
    numCols.te <- c(numCols.te,names(dt.test)[i])
  }
  else if(is.character(dt.test[,i])){
    charCols.te <- c(charCols.te,names(dt.test)[i])
  }
  else {
    misc.te <- c(misc.te,names(dt.test)[i])
  }
}

mis.vars.te <-data.frame(colSums(sapply(dt.test, is.na)))
mis.vars.te$num_yn <- "Z"
for(i in 1:nrow(mis.vars.te)){
  if( rownames(mis.vars.te)[i] %in% numCols.te){
    mis.vars.te[i,2] <- "Y"
  }else{
    mis.vars.te[i,2] <- "N"
  }
}

colnames(mis.vars.te) <- c('No_of_NAs')
kable(subset(mis.vars.te, No_of_NAs > 0 ))

```

Here, out of 80 varaibles, there are only 3 variables that has missing values. Single imputations works well in this case. So, we used Bagimpute 

NAs in character variables: All character variables contain the category of a certain feature available in the house. As per the data description from Kaggle, NAs in such cases means absence of that feature. Hence, replacing NAs with more descriptive words.

```{r Performing Imputation on numeric variables2, include=F}

bagImpute <- predict(preProcess(dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))], method = c("bagImpute")), dt.train[,which(names(dt.train) %in% c('GarageYrBlt', 'MasVnrArea', 'LotFrontage'))])
bagImpute

dt.train$GarageYrBlt <- round(bagImpute$GarageYrBlt)
dt.train$MasVnrArea <- bagImpute$MasVnrArea
dt.train$LotFrontage <- bagImpute$LotFrontage

bagImpute <- predict(preProcess(dt.test[,which(names(dt.test) %in% rownames(subset(mis.vars.te, (mis.vars.te$No_of_NAs>0 & mis.vars.te[,2] == "Y"))))], method = c("bagImpute")), dt.test[,which(names(dt.test) %in% rownames(subset(mis.vars.te, (mis.vars.te$No_of_NAs>0 & mis.vars.te[,2] == "Y"))))])
bagImpute

dt.test$GarageYrBlt <- round(bagImpute$GarageYrBlt)
dt.test$MasVnrArea <- bagImpute$MasVnrArea
dt.test$LotFrontage <- bagImpute$LotFrontage
dt.test$BsmtFinSF1 <- bagImpute$BsmtFinSF1
dt.test$BsmtFinSF2 <- bagImpute$BsmtFinSF2
dt.test$BsmtUnfSF <- bagImpute$BsmtUnfSF
dt.test$TotalBsmtSF <- bagImpute$TotalBsmtSF
dt.test$BsmtFullBath <- bagImpute$BsmtFullBath
dt.test$GarageCars <- round(bagImpute$GarageCars)
dt.test$GarageArea <- bagImpute$GarageArea

```


```{r Handling NAs in character variables, include=F}

dt.train$Alley <- ifelse(is.na(dt.train$Alley),"No Alley", dt.train$Alley)

dt.train$MasVnrType <- ifelse(is.na(dt.train$MasVnrType),"None", dt.train$MasVnrType)
# some data has masvnrtype none and area <> 0
#subset(dt.train, dt.train$MasVnrType == "None")$MasVnrArea 
dt.train$Electrical<- ifelse(is.na(dt.train$Electrical),"SBrKr",dt.train$Electrical)

dt.train$BsmtQual <- ifelse(is.na(dt.train$BsmtQual),"NoBsmt", dt.train$BsmtQual)

dt.train$BsmtCond <- ifelse(is.na(dt.train$BsmtCond),"NoBsmt", dt.train$BsmtCond)

dt.train$BsmtExposure <- ifelse(is.na(dt.train$BsmtExposure),"NoBsmt", dt.train$BsmtExposure)

dt.train$BsmtFinType1 <- ifelse(is.na(dt.train$BsmtFinType1),"NoBsmt", dt.train$BsmtFinType1)

dt.train$BsmtFinType2 <- ifelse(is.na(dt.train$BsmtFinType2),"NoBsmt", dt.train$BsmtFinType2)

dt.train$FireplaceQu <- ifelse(is.na(dt.train$FireplaceQu),"NoFireplace", dt.train$FireplaceQu)

dt.train$GarageType <- ifelse(is.na(dt.train$GarageType),"NoGarage", dt.train$GarageType)

dt.train$GarageFinish <- ifelse(is.na(dt.train$GarageFinish),"NoGarage", dt.train$GarageFinish)

dt.train$GarageQual <- ifelse(is.na(dt.train$GarageQual),"NoGarage", dt.train$GarageQual)

dt.train$GarageCond <- ifelse(is.na(dt.train$GarageCond),"NoGarage", dt.train$GarageCond)

dt.train$GarageCond <- ifelse(is.na(dt.train$GarageCond),"NoGarage", dt.train$GarageCond)

dt.train$PoolQC <- ifelse(is.na(dt.train$PoolQC),"NoPool", dt.train$PoolQC)

dt.train$Fence <- ifelse(is.na(dt.train$Fence),"NoFence", dt.train$Fence)

dt.train$MiscFeature <- ifelse(is.na(dt.train$MiscFeature),"NoFence", dt.train$MiscFeature)


dt.test$Alley <- ifelse(is.na(dt.test$Alley),"No Alley", dt.test$Alley)

dt.test$MasVnrType <- ifelse(is.na(dt.test$MasVnrType),"None", dt.test$MasVnrType)
# some data has masvnrtype none and area <> 0
#subset(dt.test, dt.test$MasVnrType == "None")$MasVnrArea 

dt.test$BsmtQual <- ifelse(is.na(dt.test$BsmtQual),"NoBsmt", dt.test$BsmtQual)

dt.test$BsmtCond <- ifelse(is.na(dt.test$BsmtCond),"NoBsmt", dt.test$BsmtCond)

dt.test$BsmtExposure <- ifelse(is.na(dt.test$BsmtExposure),"NoBsmt", dt.test$BsmtExposure)

dt.test$BsmtFinType1 <- ifelse(is.na(dt.test$BsmtFinType1),"NoBsmt", dt.test$BsmtFinType1)

dt.test$BsmtFinType2 <- ifelse(is.na(dt.test$BsmtFinType2),"NoBsmt", dt.test$BsmtFinType2)

dt.test$FireplaceQu <- ifelse(is.na(dt.test$FireplaceQu),"NoFireplace", dt.test$FireplaceQu)

dt.test$GarageType <- ifelse(is.na(dt.test$GarageType),"NoGarage", dt.test$GarageType)

dt.test$GarageFinish <- ifelse(is.na(dt.test$GarageFinish),"NoGarage", dt.test$GarageFinish)

dt.test$GarageQual <- ifelse(is.na(dt.test$GarageQual),"NoGarage", dt.test$GarageQual)

dt.test$GarageCond <- ifelse(is.na(dt.test$GarageCond),"NoGarage", dt.test$GarageCond)

dt.test$GarageCond <- ifelse(is.na(dt.test$GarageCond),"NoGarage", dt.test$GarageCond)

dt.test$PoolQC <- ifelse(is.na(dt.test$PoolQC),"NoPool", dt.test$PoolQC)

dt.test$Fence <- ifelse(is.na(dt.test$Fence),"NoFence", dt.test$Fence)

dt.test$MiscFeature <- ifelse(is.na(dt.test$MiscFeature),"NoFence", dt.test$MiscFeature)

dt.test$MSZoning <- ifelse(is.na(dt.test$MSZoning),"RM",dt.test$MSZoning)

dt.test$Utilities <- ifelse(is.na(dt.test$Utilities),"AllPub",dt.test$Utilities)

dt.test$Exterior1st <- ifelse(is.na(dt.test$Exterior1st),"WdShing",dt.test$Exterior1st)

dt.test$Exterior2nd <- ifelse(is.na(dt.test$Exterior2nd),"Stone", dt.test$Exterior2nd)

dt.test$BsmtHalfBath <- ifelse(is.na(dt.test$BsmtHalfBath),0,dt.test$BsmtHalfBath)

dt.test$KitchenQual <- ifelse(is.na(dt.test$KitchenQual),"TA",dt.test$KitchenQual)

dt.test$Functional <- ifelse(is.na(dt.test$Functional),"Typ",dt.test$Functional)

dt.test$SaleType <- ifelse(is.na(dt.test$SaleType),"WD",dt.test$SaleType)

char_var <- names(dt.train)[which(sapply(dt.train, is.character))]
for(name in char_var){
  #print(name)
  dt.train[[name]] <- factor(dt.train[[name]])
}

char_var <- names(dt.test)[which(sapply(dt.test, is.character))]
for(name in char_var){
  #print(name)
  dt.test[[name]] <- factor(dt.test[[name]])
}


```


```{r Feature Engineering, include=F}

numeric_var <- names(dt.train)[which(sapply(dt.train, is.numeric))]
df.corr <- data.frame(cor(dt.train[,(numeric_var)], method="pearson"))

#Correlation with Each variables and Sale Price:
df.sale.corr <- data.frame(abs(df.corr[,38]))
df.sale.corr$features <- names(df.corr)
#View(df.sale.corr)

df.sale.corr<- df.sale.corr[order(-df.sale.corr$abs.df.corr...38..),]
#View(df.sale.corr) #Ordered list of Correlations

#Top 5 correlated features
top5Corr <- df.sale.corr[2:7,]

```

Top 5 Correlation Numerical Variables

```{r , include=T,echo=F}

colnames(top5Corr)<- c("Cors","Features")
row.names(top5Corr) <- NULL

kable(top5Corr[,c(2,1)])

```


```{r , include=T,echo=F}
correlations <- cor(dt.train[, numeric_var], use = "everything")
corrplot(correlations, method = "circle", type="lower",  sig.level = 0.01, insig = "blank", tl.col = "grey",tl.cex = 0.6)
```

```{r inspecting multicolinearity, include=F}

df.corr <- data.frame(cor(dt.train[,(numeric_var)], method="pearson"))

df.mul.cor <- NULL 
for(i in 1:nrow(df.corr)){
  for(j in 1:i){
    df.temp <- NULL
    if(!is.na(df.corr[i,j])){
      if(df.corr[i,j] >= 0.6 && df.corr[i,j] != 1){
        df.temp$name1 <- names(df.corr)[i]
        df.temp$name2 <- names(df.corr)[j]
        df.temp$cor <- df.corr[i,j]
        df.mul.cor <- rbind(df.mul.cor,df.temp)
      }
    }
  }
}
df.mul.cor <- as.data.frame(df.mul.cor)

```
Following table contains the combinations of variables with highest correlation which has a minimum of 0.6 as corelation value. This will identify redundant predictors 

```{r, include=T, echo=F}

df.mul.cor <- subset(df.mul.cor, name1 != "SalePrice")
kable(df.mul.cor, row.names=FALSE)

```

Combining Bath into one variable 
BsmtFullbath, BsmtHalfBath, FullBath, HalfBath

```{r, include=T, echo=F}
dt.train$bathrooms <- dt.train$BsmtFullBath+ (0.5  * dt.train$BsmtHalfBath) +dt.train$FullBath+ (0.5*dt.train$HalfBath)

dt.train$porch <- dt.train$WoodDeckSF + dt.train$OpenPorchSF + dt.train$EnclosedPorch + dt.train$X3SsnPorch + dt.train$ScreenPorch

dt.train$totalRoom <- dt.train$TotRmsAbvGrd + dt.train$KitchenAbvGr


dt.test$bathrooms <- dt.test$BsmtFullBath+ (0.5  * dt.test$BsmtHalfBath) +dt.test$FullBath+ (0.5*dt.test$HalfBath)

dt.test$porch <- dt.test$WoodDeckSF + dt.test$OpenPorchSF + dt.test$EnclosedPorch + dt.test$X3SsnPorch + dt.test$ScreenPorch

dt.test$totalRoom <- dt.test$TotRmsAbvGrd + dt.test$KitchenAbvGr

```

```{r}

frmla <- formula("SalePrice ~ LotFrontage + LotArea + OverallQual + OverallCond + YearBuilt + YearRemodAdd + MasVnrArea + BsmtUnfSF + TotalBsmtSF  + GrLivArea + bathrooms + totalRoom + Fireplaces  + GarageArea + porch + YrSold")

lasso_mod <- train(frmla, data = dt.train,
                   preProcess = c("center", "scale"),
                   method = "glmnet",
                   tuneGrid= expand.grid(
                     alpha=1,
                     lambda = 0:20/20))

#plot(lasso_mod)
plot(lasso_mod$finalModel)
legend(num_coef$variables)

lasso_mod$finalModel$tuneValue
num_coef <- data.frame(as.matrix(coef(lasso_mod$finalModel, lasso_mod$finalModel$tuneValue$lambda)))
num_coef$variables <- rownames(num_coef)
num_coef$coeffiecient <- num_coef$X1
rownames(num_coef) <- NULL
num_coef$X1 <- NULL
num_coef[order(-abs(num_coef$coeffiecient)),]

rmse(dt.train$SalePrice,predict(lasso_mod, newdata = dt.train))

RMSLE <- function(A, P) {
    return (sqrt(sum((log(P+1)-log(A+1))^2)/length(A)))
}
RMSLE(dt.train$SalePrice,predict(lasso_mod, newdata = dt.train))

ridge_mod <- train(frmla, 
                data = dt.train,
                preProcess = c("center", "scale"),
                method = "glmnet",
                tuneGrid= expand.grid(
                  alpha=0,
                  lambda = 0:10))
#plot(ridge_mod)

lm_mod <- train(frmla, 
                data = dt.train,
               preProcess = c("center", "scale"),
                method = "lm")

compare_temp  <- data.frame(variables = as.character(names(coef(lm_mod$finalModel))))

compare_temp$method <- rep("lasso")
compare_temp$coefs <- c(as.numeric(as.character(coef(lasso_mod$finalModel, 
                                                     lasso_mod$finalModel$tuneValue$lambda))))

compare <- data.frame(variables = rep(as.character(names(coef(lm_mod$finalModel))),2))
compare$method <- c(rep("ridge"), rep("lm"))
compare$coefs <- c(as.numeric(as.character(coef(ridge_mod$finalModel, ridge_mod$bestTune$lambda))),
                   as.numeric(as.character(coef(lm_mod$finalModel))))

compare <- rbind(compare, compare_temp)

ggplot(compare, aes(variables, coefs, group=method, fill=method)) + 
  geom_bar(stat = "identity",position="dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```

Creating model ensemble 

```{r}
charCols.tr 

tempF <- NULL
for(name in charCols.tr){
  tempF <- paste(tempF, "+", name)
}
tempF

table(is.na(dt.train$SalePrice))

frmla = formula("SalePrice ~ LotFrontage + LotArea + OverallQual + OverallCond + YearBuilt + YearRemodAdd + MasVnrArea + BsmtUnfSF + TotalBsmtSF  + GrLivArea + bathrooms + totalRoom + Fireplaces  + GarageArea + porch + YrSold+ MSZoning + LotShape + LandContour + Utilities + LotConfig + LandSlope + Neighborhood + Condition1 + Condition2 + BldgType + HouseStyle + RoofStyle + RoofMatl + Exterior1st + Exterior2nd + MasVnrType + ExterQual + Foundation + BsmtQual + BsmtCond + BsmtExposure + BsmtFinType1 + BsmtFinType2 + Heating + HeatingQC  + KitchenQual + Functional + FireplaceQu + GarageType + GarageFinish + GarageQual + GarageCond + PavedDrive + PoolQC + Fence + MiscFeature ")



model_list <- caretList(frmla, 
                        data=dt.train,
                        trControl= trainControl(savePredictions = "final"),
                        preProcess=c("center","scale"),
                        methodList=c("rpart", "glmnet","gbm", "knn", "ranger")
                        )

summary(resamples(model_list))
modelCor(resamples(model_list))

ce <- caretEnsemble(model_list)
summary(ce)

rmse(dt.train$SalePrice,predict(ce, newdata = dt.train))
result <- predict(ce, newdata = dt.test)

dt.result <- NULL
dt.result$Id <- dt.test$Id
dt.result$SalePrice <- result

str(dt.test)
str(result)

dt.result <- as.data.frame(dt.result, row.names = NULL)

write.csv(dt.result, 'Result.csv')
```
